<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>阿里P8面试官：如何设计一个扛住千万级并发的架构（超级详细）-续</title>
      <link href="/posts/2734400627/"/>
      <url>/posts/2734400627/</url>
      
        <content type="html"><![CDATA[<p>在上一篇文章中，详细分析了设计一个千万级并发架构所需要思考的问题，以及解决方案。<br>在这一片文章中，我们主要分析如何在职场足够用户数量的情况下，同步提升架构的性能降低平均响应时间。</p><h1 id="如何降低RT的值"><a href="#如何降低RT的值" class="headerlink" title="如何降低RT的值"></a>如何降低RT的值</h1><p>继续看上面这个图，一个请求只有等到tomcat容器中的应用执行完成才能返回，而请求在执行过程中会做什么事情呢？</p><ul><li>查询数据库</li><li>访问磁盘数据</li><li>进行内存运算</li><li>调用远程服务</li></ul><p>这些操作每一个步骤都会消耗时间，当前客户端的请求只有等到这些操作都完成之后才能返回，所以降低RT的方法，就是优化业务逻辑的处理。</p><h2 id="数据库瓶颈的优化"><a href="#数据库瓶颈的优化" class="headerlink" title="数据库瓶颈的优化"></a>数据库瓶颈的优化</h2><p>当18000个请求进入到服务端并且被接收后，开始执行业务逻辑处理，那么必然会查询数据库。</p><p>每个请求至少都有一次查询数据库的操作，多的需要查询3~5次以上，我们假设按照3次来计算，那么每秒会对数据库形成54000个请求，假设一台数据库服务器每秒支撑10000个请求（影响数据库的请求数量有很多因素，比如数据库表的数据量、数据库服务器本身的系统性能、查询语句的复杂度），那么需要6台数据库服务器才能支撑每秒10000个请求。</p><p>除此之外，数据库层面还有涉及到其他的优化方案。</p><ul><li><p>首先是Mysql的最大连接数设置，大家可能遇到过<code>MySQL: ERROR 1040: Too many connections</code>这样的问题，原因就是访问量过高，连接数耗尽了。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">show</span> variables <span class="keyword">like</span> <span class="string">&#x27;%max_connections%&#x27;</span>;</span><br></pre></td></tr></table></figure><p>如果服务器的并发连接请求量比较大，建议调高此值，以增加并行连接数量，当然这建立在机器能支撑的情况下，因为如果连接数越多，介于MySQL会为每个连接提供连接缓冲区，就会开销越多的内存，所以要适当调整该值，不能盲目提高设值。</p></li><li><p>数据表数据量过大，比如达到几千万甚至上亿，这种情况下sql的优化已经毫无意义了，因为这么大的数据量查询必然会涉及到运算。</p><ul><li><p>可以缓存来解决读请求并发过高的问题，一般来说对于数据库的读写请求也都遵循2/8法则，在每秒54000个请求中，大概有43200左右是读请求，这些读请求中基本上90%都是可以通过缓存来解决。</p></li><li><p>分库分表，减少单表数据量，单表数据量少了，那么查询性能就自然得到了有效的提升</p></li><li><p>读写分离，避免事务操作对查询操作带来的性能影响</p><blockquote><ul><li><p>写操作本身耗费资源</p><p>数据库写操作为IO写入，写入过程中通常会涉及唯一性校验、建索引、索引排序等操作，对资源消耗比较大。一次写操作的响应时间往往是读操作的几倍甚至几十倍。</p></li><li><p>锁争用</p><p>写操作很多时候需要加锁，包括表级锁、行级锁等，这类锁都是排他锁，一个会话占据排它锁之后，其他会话是不能读取数据的，这会会极大影响数据读取性能。</p><p>所以MYSQL部署往往会采用读写分离方式，主库用来写入数据及部分时效性要求很高的读操作，从库用来承接大部分读操作，这样数据库整体性能能够得到大幅提升。</p></li></ul></blockquote></li></ul></li><li><p>不同类型的数据采用不同的存储库，</p><ul><li>MongoDB  nosql 文档化存储</li><li>Redis  nosql  key-value存储</li><li>HBase nosql， 列式存储，其实本质上有点类似于key-value数据库。</li><li>cassandra，Cassandra 是一个来自 Apache 的分布式数据库，具有高度可扩展性，可用于管理大量的结构化数据</li><li>TIDB，是PingCAP公司自主设计、研发的开源分布式关系型数据库，是一款同时支持在线事务处理与在线分析处理 (Hybrid Transactional and Analytical Processing, HTAP) 的融合型分布式数据库产品</li></ul></li></ul><blockquote><p>为什么把mysql数据库中的数据放redis缓存中能提升性能？</p><ol><li>Redis存储的是k-v格式的数据。时间复杂度是O(1),常数阶,而mysql引擎的底层实现是B+TREE，时间复杂度是O(logn）是对数阶的。Redis会比Mysql快一点点。</li><li>Mysql数据存储是存储在表中，查找数据时要先对表进行全局扫描或根据索引查找，这涉及到磁盘的查找，磁盘查找如果是单点查找可能会快点，但是顺序查找就比较慢。而redis不用这么麻烦，本身就是存储在内存中，会根据数据在内存的位置直接取出。</li><li>Redis是单线程的多路复用IO,单线程避免了线程切换的开销，而多路复用IO避免了IO等待的开销，在多核处理器下提高处理器的使用效率可以对数据进行分区，然后每个处理器处理不同的数据。</li></ol></blockquote><ul><li><p>池化技术，减少频繁创建数据库连接的性能损耗。</p><p>每次进行数据库操作之前，先建立连接然后再进行数据库操作，最后释放连接。这个过程涉及到网络通信的延时，频繁创建连接对象和销毁对象的性能开销等，当请求量较大时，这块带来的性能影响非常大。</p></li></ul><p><img src="https://mic-blob-bucket.oss-cn-beijing.aliyuncs.com/202110171350704.png" alt="数据存储"></p><h2 id="磁盘数据访问优化"><a href="#磁盘数据访问优化" class="headerlink" title="磁盘数据访问优化"></a>磁盘数据访问优化</h2><p>对于磁盘的操作，无非就是读和写。</p><p>比如对于做交易系统的场景来说，一般会设计到对账文件的解析和写入。而对于磁盘的操作，优化方式无非就是</p><ul><li><p>磁盘的页缓存，可以借助缓存 I/O ，充分利用系统缓存，降低实际 I/O 的次数。</p></li><li><p>顺序读写，可以用追加写代替随机写，减少寻址开销，加快 I/O 写的速度。</p></li><li><p>SSD代替HDD，固态硬盘的I/O效率远远高于机械硬盘。</p></li><li><p>在需要频繁读写同一块磁盘空间时，可以用 mmap （内存映射，）代替 read/write，减少内存的拷贝次数</p></li><li><p>在需要同步写的场景中，尽量将写请求合并，而不是让每个请求都同步写入磁盘，即可以用 fsync() 取代 O_SYNC</p></li></ul><h2 id="合理利用内存"><a href="#合理利用内存" class="headerlink" title="合理利用内存"></a>合理利用内存</h2><p>充分利用内存缓存，把一些经常访问的数据和对象保存在内存中，这样可以避免重复加载或者避免数据库访问带来的性能损耗。</p><h2 id="调用远程服务"><a href="#调用远程服务" class="headerlink" title="调用远程服务"></a>调用远程服务</h2><p>远程服务调用，影响到IO性能的因素有。</p><ul><li>远程调用等待返回结果的阻塞<ul><li>异步通信</li></ul></li><li>网络通信的耗时<ul><li>内网通信</li><li>增加网络带宽</li></ul></li><li>远程服务通信的稳定性</li></ul><h2 id="异步化架构"><a href="#异步化架构" class="headerlink" title="异步化架构"></a>异步化架构</h2><p>微服务中的逻辑复杂处理时间长的情况，在高并发量下，导致服务线程消耗尽，不能再创建线程处理请求。对这种情况的优化，除了在程序上不断调优(数据库调优，算法调优，缓存等等)，可以考虑在架构上做些调整，先返回结果给客户端，让用户可以继续使用客户端的其他操作，再把服务端的复杂逻辑处理模块做异步化处理。这种异步化处理的方式适合于客户端对处理结果不敏感不要求实时的情况，比如群发邮件、群发消息等。</p><p>异步化设计的解决方案： 多线程、MQ。</p><h1 id="应用服务的拆分"><a href="#应用服务的拆分" class="headerlink" title="应用服务的拆分"></a>应用服务的拆分</h1><p>除了上述的手段之外，业务系统往微服务化拆分也非常有必要，原因是：</p><ul><li>随着业务的发展，应用程序本身的复杂度会不断增加，同样会产生熵增现象。</li><li>业务系统的功能越来越多，参与开发迭代的人员也越多，多个人维护一个非常庞大的项目，很容易出现问题。</li><li>单个应用系统很难实现横向扩容，并且由于服务器资源有限，导致所有的请求都集中请求到某个服务器节点，造成资源消耗过大，使得系统不稳定</li><li>测试、部署成本越来越高</li><li>…..</li></ul><p>其实，最终要的是，单个应用在性能上的瓶颈很难突破，也就是说如果我们要支持18000QPS，单个服务节点肯定无法支撑，所以服务拆分的好处，就是可以利用多个计算机阶段组成一个大规模的分布式计算网络，通过网络通信的方式完成一整套业务逻辑。</p><p><img src="https://mic-blob-bucket.oss-cn-beijing.aliyuncs.com/202110171350400.png" alt="img"></p><h2 id="如何拆分服务"><a href="#如何拆分服务" class="headerlink" title="如何拆分服务"></a>如何拆分服务</h2><p>如何拆分服务，这个问题看起来简单，很多同学会说，直接按照业务拆分啊。</p><p>但是实际在实施的时候，会发现拆分存在一些边界性问题，比如有些数据模型可以存在A模块，也可以存在B模块，这个时候怎么划分呢？另外，服务拆分的粒度应该怎么划分？</p><p>一般来说，服务的拆分是按照业务来实现的，然后基于DDD来指导微服务的边界划分。<strong>领域驱动就是一套方法论，通过领域驱动设计方法论来定义领域模型，从而确定业务边界和应用边界，保证业务模型和代码模型的一致性。</strong>不管是DDD还是微服务，都要遵循软件设计的基本原则：<strong>高内聚低耦合</strong>。服务内部高内聚，服务之间低耦合，实际上一个领域服务对应了一个功能集合，这些功能一定是有一些共性的。比如，订单服务，那么创建订单、修改订单、查询订单列表，领域的边界越清晰，功能也就越内聚，服务之间的耦合性也就越低。</p><p>服务拆分还需要根据当前技术团队和公司所处的状态来进行。</p><p>如果是初创团队，不需要过分的追求微服务，否则会导致业务逻辑过于分散，技术架构太过负载，再加上团队的基础设施还不够完善，导致整个交付的时间拉长，对公司的发展来说会造成较大的影响。所以在做服务拆分的时候还需要考虑几个因素。</p><ul><li>当前公司业务所处领域的市场性质，如果是市场较为敏感的项目，前期应该是先出来东西，然后再去迭代和优化。</li><li>开发团队的成熟度，团队技术能否能够承接。</li><li>基础能力是否足够，比如Devops、运维、测试自动化等基础能力。 团队是否有能力来支撑大量服务实例运行带来的运维复杂度，是否可以做好服务的监控。</li><li>测试团队的执行效率，如果测试团队不能支持自动化测试、自动回归、压力测试等手段来提高测试效率，那必然会带来测试工作量的大幅度提升从而导致项目上线周期延期</li></ul><p>如果是针对一个老的系统进行改造，那可能涉及到的风险和问题更多，所以要开始着手改动之前，需要考虑几个步骤：拆分前准备阶段，设计拆分改造方案，实施拆分计划</p><ul><li><p>拆分之前，先梳理好当前的整个架构，以及各个模块的依赖关系，还有接口</p><p>准备阶段主要是梳理清楚了依赖关系和接口，就可以思考如何来拆，第一刀切在哪儿里，即能达到快速把一个复杂单体系统变成两个更小系统的目标，又能对系统的现有业务影响最小。要尽量避免构建出一个分布式的单体应用，一个包含了一大堆互相之间紧耦合的服务，却又必须部署在一起的所谓分布式系统。没分析清楚就强行拆，可能就一不小心剪断了大动脉，立马搞出来一个 A 类大故障，后患无穷。</p></li><li><p>不同阶段拆分要点不同，每个阶段的关注点要聚焦</p><p>拆分本身可以分成三个阶段，核心业务和非业务部分的拆分、核心业务的调整设计、核心业务内部的拆分。</p><ul><li><p>第一阶段将核心业务瘦身，把非核心的部分切开，减少需要处理的系统大小；</p></li><li><p>第二阶段。重新按照微服务设计核心业务部分；</p></li><li><p>第三阶段把核心业务部分重构设计落地。</p></li></ul><p>拆分的方式也有三个：代码拆分、部署拆分、数据拆分。</p></li></ul><p>另外，每个阶段需要聚焦到一两个具体的目标，否则目标太多反而很难把一件事儿做通透。例如某个系统的微服务拆分，制定了如下的几个目标：</p><ol><li>性能指标（吞吐和延迟）：核心交易吞吐提升一倍以上（TPS：1000-&gt;10000），A 业务延迟降低一半（Latency：250ms-&gt;125ms），B 业务延迟降低一半（Latency：70ms-&gt;35ms）。</li><li>稳定性指标（可用性，故障恢复时间）：可用性&gt;=99.99%，A 类故障恢复时间&lt;=15 分钟，季度次数&lt;=1 次。</li><li>质量指标：编写完善的产品需求文档、设计文档、部署运维文档，核心交易部分代码 90%以上单测覆盖率和 100%的自动化测试用例和场景覆盖，实现可持续的性能测试基准环境和长期持续性能优化机制。</li><li>扩展性指标：完成代码、部署、运行时和数据多个维度的合理拆分，对于核心系统重构后的各块业务和交易模块、以及对应的各个数据存储，都可以随时通过增加机器资源实现伸缩扩展。</li><li>可维护性指标：建立全面完善的监控指标、特别是全链路的实时性能指标数据，覆盖所有关键业务和状态，缩短监控报警响应处置时间，配合运维团队实现容量规划和管理，出现问题时可以在一分钟内拉起系统或者回滚到上一个可用版本（启动时间&lt;=1 分钟）。</li><li>易用性指标，通过重构实现新的 API 接口既合理又简单，极大的满足各个层面用户的使用和需要，客户满意度持续上升。</li><li>业务支持指标：对于新的业务需求功能开发，在保障质量的前提下，开发效率提升一倍，开发资源和周期降低一半。</li></ol><p>当然，不要期望一次性完成所有目标，每一个阶段可以选择一个两个优先级高的目标进行执行。</p><p><img src="https://mic-blob-bucket.oss-cn-beijing.aliyuncs.com/202110171350615.png" alt="img"></p><h2 id="微服务化架构带来的问题"><a href="#微服务化架构带来的问题" class="headerlink" title="微服务化架构带来的问题"></a>微服务化架构带来的问题</h2><p>微服务架构首先是一个分布式的架构，其次我们要暴露和提供业务服务能力，然后我们需要考虑围绕这些业务能力的各种非功能性的能力。这些分散在各处的服务本身需要被管理起来，并且对服务的调用方透明，这样就有了服务的注册发现的功能需求。</p><p>同样地，每个服务可能部署了多台机器多个实例，所以，我们需要有路由和寻址的能力，做负载均衡，提升系统的扩展能力。有了这么多对外提供的不同服务接口，我们一样需要有一种机制对他们进行统一的接入控制，并把一些非业务的策略做到这个接入层，比如权限相关的，这就是服务网关。同时我们发现随着业务的发展和一些特定的运营活动，比如秒杀大促，流量会出现十倍以上的激增，这时候我们就需要考虑系统容量，服务间的强弱依赖关系，做服务降级、熔断，系统过载保护等措施。</p><p>以上这些由于微服务带来的复杂性，导致了应用配置、业务配置，都被散落到各处，所以分布式配置中心的需求也出现了。最后，系统分散部署以后，所有的调用都跨了进程，我们还需要有能在线上做链路跟踪，性能监控的一套技术，来协助我们时刻了解系统内部的状态和指标，让我们能够随时对系统进行分析和干预。</p><p><img src="https://mic-blob-bucket.oss-cn-beijing.aliyuncs.com/202110171351673.png" alt="image-20210624133950124"></p><h2 id="整体架构图"><a href="#整体架构图" class="headerlink" title="整体架构图"></a>整体架构图</h2><p>基于上述从微观到宏观的整体分析，我们基本上能够设计出一个整体的架构图。</p><ul><li><p>接入层，外部请求到内部系统之间的关口，所有请求都必须经过api 网关。</p></li><li><p>应用层，也叫聚合层，为相关业务提供聚合接口，它会调用中台服务进行组装。</p></li><li><p>中台服务，也是业务服务层，以业务为纬度提供业务相关的接口。中台的本质是为整个架构提供复用的能力，比如评论系统，在咕泡云课堂和Gper社区都需要，那么这个时候评论系统为了设计得更加可复用性，就不能耦合云课堂或者Gper社区定制化的需求，那么作为设计评论中台的人，就不需要做非常深度的思考，如何提供一种针对不同场景都能复用的能力。</p><p>你会发现，当这个服务做到机制的时候，就变成了一个baas服务。</p><blockquote><p><strong>服务商</strong>为<strong>客户</strong>(开发者)提供整合云后端的服务，如提供文件存储、数据存储、推送服务、身份验证服务等功能，以帮助开发者快速开发应用。</p></blockquote></li></ul><p><img src="https://mic-blob-bucket.oss-cn-beijing.aliyuncs.com/202110091513908.png" alt="image-20210624152616146"></p><h1 id="了解什么是高并发"><a href="#了解什么是高并发" class="headerlink" title="了解什么是高并发"></a>了解什么是高并发</h1><p>总结一下什么是高并发。</p><p>高并发并没有一个具体的定义，高并发主要是形容突发流量较高的场景。</p><p>如果面试的过程中，或者在实际工作中，你们领导或者面试官问你一个如何设计承接千万级流量的系统时，你应该要按照我说的方法去进行逐一分析。</p><ul><li>一定要形成可以量化的数据指标，比如QPS、DAU、总用户数、TPS、访问峰值</li><li>针对这些数据情况，开始去设计整个架构方案</li><li>接着落地执行</li></ul><h2 id="高并发中的宏观指标"><a href="#高并发中的宏观指标" class="headerlink" title="高并发中的宏观指标"></a>高并发中的宏观指标</h2><p>一个满足高并发系统，不是一味追求高性能，至少需要满足三个宏观层面的目标：</p><ul><li>高性能，性能体现了系统的并行处理能力，在有限的硬件投入下，提高性能意味着节省成本。同时，性能也反映了用户体验，响应时间分别是 100 毫秒和 1 秒，给用户的感受是完全不同的。</li><li>高可用，表示系统可以正常服务的时间。一个全年不停机、无故障；另一个隔三差五出现上事故、宕机，用户肯定选择前者。另外，如果系统只能做到 90%可用，也会大大拖累业务。</li><li>高扩展，表示系统的扩展能力，流量高峰时能否在短时间内完成扩容，更平稳地承接峰值流量，比如双 11 活动、明星离婚等热点事件。</li></ul><p><img src="https://mic-blob-bucket.oss-cn-beijing.aliyuncs.com/202110091513525.png" alt="image-20210624211728937"></p><h2 id="微观指标"><a href="#微观指标" class="headerlink" title="微观指标"></a>微观指标</h2><p><strong>性能指标</strong></p><p>通过性能指标可以度量目前存在的性能问题，同时作为性能优化的评估依据。一般来说，会采用一段时间内的接口响应时间作为指标。</p><p>1、平均响应时间：最常用，但是缺陷很明显，对于慢请求不敏感。比如 1 万次请求，其中 9900 次是 1ms，100 次是 100ms，则平均响应时间为 1.99ms，虽然平均耗时仅增加了 0.99ms，但是 1%请求的响应时间已经增加了 100 倍。</p><p>2、TP90、TP99 等分位值：将响应时间按照从小到大排序，TP90 表示排在第 90 分位的响应时间， 分位值越大，对慢请求越敏感。</p><p><img src="https://mic-blob-bucket.oss-cn-beijing.aliyuncs.com/202110091513882.jpeg" alt="img"></p><p><strong>可用性指标</strong></p><p>高可用性是指系统具有较高的无故障运行能力，可用性 = 平均故障时间 / 系统总运行时间，一般使用几个 9 来描述系统的可用性。</p><p>对于高并发系统来说，最基本的要求是：保证 3 个 9 或者 4 个 9。原因很简单，如果你只能做到 2 个 9，意味着有 1%的故障时间，像一些大公司每年动辄千亿以上的 GMV 或者收入，1%就是 10 亿级别的业务影响。</p><p><strong>可扩展性指标</strong></p><p>面对突发流量，不可能临时改造架构，最快的方式就是增加机器来线性提高系统的处理能力。</p><p>对于业务集群或者基础组件来说，扩展性 = 性能提升比例 / 机器增加比例，理想的扩展能力是：资源增加几倍，性能提升几倍。通常来说，扩展能力要维持在 70%以上。</p><p>但是从高并发系统的整体架构角度来看，扩展的目标不仅仅是把服务设计成无状态就行了，因为当流量增加 10 倍，业务服务可以快速扩容 10 倍，但是数据库可能就成为了新的瓶颈。</p><p>像 MySQL 这种有状态的存储服务通常是扩展的技术难点，如果架构上没提前做好规划（垂直和水平拆分），就会涉及到大量数据的迁移。</p><p>因此，高扩展性需要考虑：服务集群、数据库、缓存和消息队列等中间件、负载均衡、带宽、依赖的第三方等，当并发达到某一个量级后，上述每个因素都可能成为扩展的瓶颈点。</p><h2 id="实践方案"><a href="#实践方案" class="headerlink" title="实践方案"></a>实践方案</h2><p>通用设计方法</p><p><strong>纵向扩展（scale-up）</strong></p><p>它的目标是提升单机的处理能力，方案又包括：</p><p>1、提升单机的硬件性能：通过增加内存、CPU 核数、存储容量、或者将磁盘升级成 SSD 等堆硬件的方式来提升。</p><p>2、提升单机的软件性能：使用缓存减少 IO 次数，使用并发或者异步的方式增加吞吐量。</p><p><strong>横向扩展（scale-out）</strong></p><p>因为单机性能总会存在极限，所以最终还需要引入横向扩展，通过集群部署以进一步提高并发处理能力，又包括以下 2 个方向：</p><p>1、做好分层架构：这是横向扩展的提前，因为高并发系统往往业务复杂，通过分层处理可以简化复杂问题，更容易做到横向扩展。</p><p>2、各层进行水平扩展：无状态水平扩容，有状态做分片路由。业务集群通常能设计成无状态的，而数据库和缓存往往是有状态的，因此需要设计分区键做好存储分片，当然也可以通过主从同步、读写分离的方案提升读性能。</p><h3 id="高性能实践方案"><a href="#高性能实践方案" class="headerlink" title="高性能实践方案"></a>高性能实践方案</h3><p>1、集群部署，通过负载均衡减轻单机压力。</p><p>2、多级缓存，包括静态数据使用 CDN、本地缓存、分布式缓存等，以及对缓存场景中的热点 key、缓存穿透、缓存并发、数据一致性等问题的处理。</p><p>3、分库分表和索引优化，以及借助搜索引擎解决复杂查询问题。</p><p>4、考虑 NoSQL 数据库的使用，比如 HBase、TiDB 等，但是团队必须熟悉这些组件，且有较强的运维能力。</p><p>5、异步化，将次要流程通过多线程、MQ、甚至延时任务进行异步处理。</p><p>6、限流，需要先考虑业务是否允许限流（比如秒杀场景是允许的），包括前端限流、Nginx 接入层的限流、服务端的限流。</p><p>7、对流量进行削峰填谷，通过 MQ 承接流量。</p><p>8、并发处理，通过多线程将串行逻辑并行化。</p><p>9、预计算，比如抢红包场景，可以提前计算好红包金额缓存起来，发红包时直接使用即可。</p><p>10、缓存预热，通过异步任务提前预热数据到本地缓存或者分布式缓存中。</p><p>11、减少 IO 次数，比如数据库和缓存的批量读写、RPC 的批量接口支持、或者通过冗余数据的方式干掉 RPC 调用。</p><p>12、减少 IO 时的数据包大小，包括采用轻量级的通信协议、合适的数据结构、去掉接口中的多余字段、减少缓存 key 的大小、压缩缓存 value 等。</p><p>13、程序逻辑优化，比如将大概率阻断执行流程的判断逻辑前置、For 循环的计算逻辑优化，或者采用更高效的算法。</p><p>14、各种池化技术的使用和池大小的设置，包括 HTTP 请求池、线程池（考虑 CPU 密集型还是 IO 密集型设置核心参数）、数据库和 Redis 连接池等。</p><p>15、JVM 优化，包括新生代和老年代的大小、GC 算法的选择等，尽可能减少 GC 频率和耗时。</p><p>16、锁选择，读多写少的场景用乐观锁，或者考虑通过分段锁的方式减少锁冲突。</p><h3 id="高可用实践方案"><a href="#高可用实践方案" class="headerlink" title="高可用实践方案"></a>高可用实践方案</h3><p>1、对等节点的故障转移，Nginx 和服务治理框架均支持一个节点失败后访问另一个节点。</p><p>2、非对等节点的故障转移，通过心跳检测并实施主备切换（比如 redis 的哨兵模式或者集群模式、MySQL 的主从切换等）。</p><p>3、接口层面的超时设置、重试策略和幂等设计。</p><p>4、降级处理：保证核心服务，牺牲非核心服务，必要时进行熔断；或者核心链路出问题时，有备选链路。</p><p>5、限流处理：对超过系统处理能力的请求直接拒绝或者返回错误码。</p><p>6、MQ 场景的消息可靠性保证，包括 producer 端的重试机制、broker 侧的持久化、consumer 端的 ack 机制等。</p><p>7、灰度发布，能支持按机器维度进行小流量部署，观察系统日志和业务指标，等运行平稳后再推全量。</p><p>8、监控报警：全方位的监控体系，包括最基础的 CPU、内存、磁盘、网络的监控，以及 Web 服务器、JVM、数据库、各类中间件的监控和业务指标的监控。</p><p>9、灾备演练：类似当前的“混沌工程”，对系统进行一些破坏性手段，观察局部故障是否会引起可用性问题。</p><p>高可用的方案主要从冗余、取舍、系统运维 3 个方向考虑，同时需要有配套的值班机制和故障处理流程，当出现线上问题时，可及时跟进处理。</p><h3 id="高扩展的实践方案"><a href="#高扩展的实践方案" class="headerlink" title="高扩展的实践方案"></a>高扩展的实践方案</h3><p>1、合理的分层架构：比如上面谈到的互联网最常见的分层架构，另外还能进一步按照数据访问层、业务逻辑层对微服务做更细粒度的分层（但是需要评估性能，会存在网络多一跳的情况）。</p><p>2、存储层的拆分：按照业务维度做垂直拆分、按照数据特征维度进一步做水平拆分（分库分表）。</p><p>3、业务层的拆分：最常见的是按照业务维度拆（比如电商场景的商品服务、订单服务等），也可以按照核心接口和非核心接口拆，还可以按照请求去拆（比如 To C 和 To B，APP 和 H5）。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 高并发 </tag>
            
            <tag> 面试题 </tag>
            
            <tag> 架构设计 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>阿里P8面试官：如何设计一个扛住千万级并发的架构？</title>
      <link href="/posts/1568521894/"/>
      <url>/posts/1568521894/</url>
      
        <content type="html"><![CDATA[<hr><p>title: 阿里P8面试官：如何设计一个扛住千万级并发的架构？<br>top_img: <a href="http://up.36992.com/pic/e6/c5/51/e6c551e768092c8655292d89a4034a74.jpg">http://up.36992.com/pic/e6/c5/51/e6c551e768092c8655292d89a4034a74.jpg</a><br>cover:<a href="http://up.36992.com/pic/e6/c5/51/e6c551e768092c8655292d89a4034a74.jpg">http://up.36992.com/pic/e6/c5/51/e6c551e768092c8655292d89a4034a74.jpg</a></p><hr><p>大家先思考一个问题，这也是在面试过程中经常遇到的问题。</p><blockquote><p>如果你们公司现在的产品能够支持10W用户访问，你们老板突然和你说，融到钱了，会大量投放广告，预计在1个月后用户量会达到1000W，如果这个任务交给你，你应该怎么做？</p></blockquote><h1 id="1000W用户的问题分解"><a href="#1000W用户的问题分解" class="headerlink" title="1000W用户的问题分解"></a>1000W用户的问题分解</h1><p>如何支撑1000W用户其实是一个非常抽象的问题，对于技术开发来说，我们需要一个非常明确的对于执行关键业务上的性能指标数据，比如，高峰时段下对于事务的响应时间、并发用户数、QPS、成功率、以及基本指标要求等，这些都 必须要非常明确，只有这样才能够指导整个架构的改造和优化。所以，如果大家接到这样一个问题，首先需要去定位到问题的本质，也就是首先得知道一些可量化的数据指标。</p><ul><li><p>如果有过往的相似业务交易历史数据经验，你需要尽量参考，处理这些收集到的原始数据（日志），从而分析出高峰时段，以及该时段下的交易行为，交易规模等，得到你想要看清楚的需求细节</p></li><li><p>另外一种情况，就是没有相关的数据指标作为参考，这个时候就需要经验来分析。比如可以参考一些类似行业的比较成熟的业务交易模型（比如银行业的日常交易活动或交通行业售检票交易活动）或者干脆遵循“2/8”原则和“2/5/8”原则来直接下手实践。</p><blockquote><ul><li>当用户能够在2秒以内得到响应时，会感觉系统的响应很快；</li><li>当用户在2-5秒之间得到响应时，会感觉系统的响应速度还可以；</li><li>当用户在5-8秒以内得到响应时，会感觉系统的响应速度很慢，但是还可以接受；</li><li>而当用户在超过8秒后仍然无法得到响应时，会感觉系统糟透了，或者认为系统已经失去响应，而选择离开这个Web站点，或者发起第二次请求。</li></ul></blockquote></li></ul><p>在估算响应时间、并发用户数、TPS、成功率这些关键指标的同时，你仍需要关心具体的业务功能维度上的需求，每个业务功能都有各自的特点，比如有些场景可以不需要同步返回明确执行结果，有些业务场景可以接受返回“系统忙，请等待！”这样暴力的消息，以避免过大的处理流量所导致的大规模瘫痪，因此，学会平衡这些指标之间的关系是必要的，大多数情况下最好为这些指标做一个优先级排序，并且尽量只考察几个优先级高的指标要求。(SLA服务等级)</p><blockquote><p><strong>SLA</strong>：Service-Level Agreement的缩写，意思是服务等级协议。服务的SLA是服务提供者对服务消费者的正式承诺，是衡量服务能力等级的关键项。服务SLA中定义的项必须是可测量的，有明确的测量方法。</p></blockquote><p><img src="https://mic-blob-bucket.oss-cn-beijing.aliyuncs.com/202110171350947.png" alt="image-20210623165109183"></p><h2 id="并发中相关概念的解释"><a href="#并发中相关概念的解释" class="headerlink" title="并发中相关概念的解释"></a>并发中相关概念的解释</h2><p>在分析上述问题之前，先给大家普及一下，系统相关的一些关键衡量指标。</p><h3 id="TPS"><a href="#TPS" class="headerlink" title="TPS"></a>TPS</h3><p>TPS（Transaction Per Second）每秒处理的事务数。</p><p>站在宏观角度来说，一个事务是指客户端向服务端发起一个请求，并且等到请求返回之后的整个过程。从客户端发起请求开始计时，等到收到服务器端响应结果后结束计时，在计算这个时间段内总共完成的事务个数，我们称为TPS。</p><p>站在微观角度来说，一个数据库的事务操作，从开始事务到事务提交完成，表示一个完整事务，这个是数据库层面的TPS。</p><h3 id="QPS"><a href="#QPS" class="headerlink" title="QPS"></a>QPS</h3><p>QPS（Queries Per Second）每秒查询数，表示服务器端每秒能够响应的查询次数。这里的查询是指用户发出请求到服务器做出响应成功的次数，可以简单认为每秒钟的Request数量。</p><p>针对单个接口而言，TPS和QPS是相等的。如果从宏观层面来说，用户打开一个页面到页面渲染结束代表一个TPS，那这个页面中会调用服务器很多次，比如加载静态资源、查询服务器端的渲染数据等，就会产生两个QPS，因此，一个TPS中可能会包含多个QPS。</p><blockquote><p><strong>QPS=并发数/平均响应时间</strong></p></blockquote><p><img src="https://mic-blob-bucket.oss-cn-beijing.aliyuncs.com/202110171350727.png" alt="image-20210622180649041"></p><h3 id="RT"><a href="#RT" class="headerlink" title="RT"></a>RT</h3><p>RT（Response Time），表示客户端发起请求到服务端返回的时间间隔，一般表示平均响应时间。</p><h3 id="并发数"><a href="#并发数" class="headerlink" title="并发数"></a>并发数</h3><p>并发数是指系统同时能处理的请求数量。</p><p>需要注意，并发数和QPS不要搞混了，QPS表示每秒的请求数量，而并发数是系统同时处理的请求数量，并发数量会大于QPS，因为服务端的一个连接需要有一个处理时长，在这个请求处理结束之前，这个连接一直占用。</p><p>举个例子，如果QPS=1000，表示每秒钟客户端会发起1000个请求到服务端，而如果一个请求的处理耗时是3s，那么意味着总的并发=1000*3=3000，也就是服务端会同时有3000个并发。</p><h3 id="计算方法"><a href="#计算方法" class="headerlink" title="计算方法"></a>计算方法</h3><p>上面说的这些指标，怎么计算呢？举个例子。</p><p>假设在10点到11点这一个小时内，有200W个用户访问我们的系统，假设平均每个用户请求的耗时是3秒，那么计算的结果如下：</p><ul><li>QPS=2000000/60*60 = 556 （表示每秒钟会有556个请求发送到服务端）</li><li>RT=3s（每个请求的平均响应时间是3秒）</li><li>并发数=556*3=1668</li></ul><p>从这个计算过程中发现，随着RT的值越大，那么并发数就越多，而并发数代表着服务器端同时处理的连接请求数量，也就意味服务端占用的连接数越多，这些链接会消耗内存资源以及CPU资源等。所以RT值越大系统资源占用越大，同时也意味着服务端的请求处理耗时较长。</p><p>但实际情况是，RT值越小越好，比如在游戏中，至少做到100ms左右的响应才能达到最好的体验，对于电商系统来说，3s左右的时间是能接受的，那么如何缩短RT的值呢？</p><h2 id="按照2-8法则来推算1000w用户的访问量"><a href="#按照2-8法则来推算1000w用户的访问量" class="headerlink" title="按照2/8法则来推算1000w用户的访问量"></a>按照2/8法则来推算1000w用户的访问量</h2><p>继续回到最开始的问题，假设没有历史数据供我们参考，我们可以使用2/8法则来进行预估。</p><ul><li><p>1000W用户，每天来访问这个网站的用户占到20%，也就是每天有200W用户来访问。</p></li><li><p>假设平均每个用户过来点击50次，那么总共的PV=1亿。</p></li><li><p>一天是24小时，根据2/8法则，每天大部分用户活跃的时间点集中在(24*0.2) 约等于5个小时以内，而大部分用户指的是（1亿点击 * 80%）约等于8000W（PV）， 意味着在5个小时以内，大概会有8000W点击进来，也就是每秒大约有4500(8000W/5小时)个请求。</p></li><li><p>4500只是一个平均数字。在这5个小时中，不可能请求是非常平均的，有可能会存在大量的用户集中访问（比如像淘宝这样的网站，日访问峰值的时间点集中在下午14：00、以及晚上21：00，其中21：00是一天中活跃的峰值），一般情况下访问峰值是平均访问请求的3倍到4倍左右（这个是经验值），我们按照4倍来计算。那么在这5个小时内有可能会出现每秒18000个请求的情况。也就是说，问题由原本的支撑1000W用户，变成了一个具体的问题，<strong>就是服务器端需要能够支撑每秒18000个请求</strong>（QPS=18000）</p></li></ul><p><img src="https://mic-blob-bucket.oss-cn-beijing.aliyuncs.com/202110171350543.png" alt="image-20210622160313561"></p><p><img src="https://mic-blob-bucket.oss-cn-beijing.aliyuncs.com/202110171350921.png" alt="image-20210622160320454"></p><h2 id="服务器压力预估"><a href="#服务器压力预估" class="headerlink" title="服务器压力预估"></a>服务器压力预估</h2><p>大概预估出了后端服务器需要支撑的最高并发的峰值之后，就需要从整个系统架构层面进行压力预估，然后配置合理的服务器数量和架构。既然是这样，那么首先需要知道一台服务器能够扛做多少的并发，那这个问题怎么去分析呢？我们的应用是部署在Tomcat上，所以需要从Tomcat本身的性能下手。</p><p>下面这个图表示Tomcat的工作原理，该图的说明如下。</p><ul><li><p>LimitLatch是连接控制器，它负责控制Tomcat能够同时处理的最大连接数，在NIO/NIO2的模式中，默认是10000，如果是APR/native，默认是8192</p></li><li><p>Acceptor是一个独立的线程，在run方法中，在while循环中调用socket.accept方法中接收客户端的连接请求，一旦有新的请求过来，accept会返回一个Channel对象，接着把这个Channel对象交给Poller去处理。</p><blockquote><p>Poller 的本质是一个 Selector ，它同样也实现了线程，Poller 在内部维护一个 Channel 数组，它在一个死循环里不断检测 Channel 的数据就绪状态，一旦有 Channel 可读，就生成一个 SocketProcessor 任务对象扔给 Executor 去处理</p></blockquote></li><li><p>SocketProcessor 实现了 Runnable 接口，当线程池在执行SocketProcessor这个任务时，会通过Http11Processor去处理当前这个请求，Http11Processor 读取 Channel 的数据来生成 ServletRequest 对象。</p></li><li><p>Executor 就是线程池，负责运行 SocketProcessor 任务类， SocketProcessor 的 run 方法会调用 Http11Processor 来读取和解析请求数据。我们知道， Http11Processor 是应用层协议的封装，它会调用容器获得响应，再把响应通过 Channel 写出。</p></li></ul><p><img src="https://mic-blob-bucket.oss-cn-beijing.aliyuncs.com/202110171350538.png" alt="image-20210622154519229"></p><p>从这个图中可以得出，限制Tomcat请求数量的因素四个方面。</p><h3 id="当前服务器系统资源"><a href="#当前服务器系统资源" class="headerlink" title="当前服务器系统资源"></a>当前服务器系统资源</h3><p>我想可能大家遇到过类似“Socket/File：Can’t open so many files”的异常，这个就是表示Linux系统中的文件句柄限制。</p><p>在Linux中，每一个TCP连接会占用一个文件描述符（fd），一旦文件描述符超过Linux系统当前的限制，就会提示这个错误。</p><p>我们可以通过下面这条命令来查看一个进程可以打开的文件数量</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ulimit -a 或者 ulimit -n</span><br></pre></td></tr></table></figure><p>open files （-n） 1024 是linux操作系统对一个进程打开的文件句柄数量的限制（也包含打开的套接字数量）</p><p>这里只是对用户级别的限制，其实还有个是对系统的总限制，查看系统总线制：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cat /proc/sys/fs/file-max</span><br></pre></td></tr></table></figure><p>file-max是设置系统所有进程一共可以打开的文件数量 。同时一些程序可以通过<code>setrlimit</code>调用，设置每个进程的限制。如果得到大量使用完文件句柄的错误信息，是应该增加这个值。</p><p>当出现上述异常时，我们可以通过下面的方式来进行修改（针对单个进程的打开数量限制）</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vi /etc/security/limits.conf</span><br><span class="line">  root soft nofile 65535</span><br><span class="line">  root hard nofile 65535</span><br><span class="line">  * soft nofile 65535</span><br><span class="line">  * hard nofile 65535</span><br></pre></td></tr></table></figure><ul><li><code>*</code>代表所有用户、<code>root</code>表示root用户。</li><li>noproc 表示最大进程数量</li><li>nofile代表最大文件打开数量。</li><li>soft/hard，前者当达到阈值时，制作警告，后者会报错。</li></ul><p>另外还要注意，要确保针对进程级别的文件打开数量反问是小于或者等于系统的总限制，否则，我们需要修改系统的总限制。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vi /proc/sys/fs/file-max</span><br></pre></td></tr></table></figure><p>TCP连接对于系统资源最大的开销就是内存。</p><p>因为tcp连接归根结底需要双方接收和发送数据，那么就需要一个读缓冲区和写缓冲区，这两个buffer在linux下最小为4096字节，可通过cat /proc/sys/net/ipv4/tcp_rmem和cat /proc/sys/net/ipv4/tcp_wmem来查看。</p><p>所以，一个tcp连接最小占用内存为4096+4096 = 8k，那么对于一个8G内存的机器，在不考虑其他限制下，最多支持的并发量为：8<em>1024</em>1024/8 约等于100万。此数字为纯理论上限数值，在实际中，由于linux kernel对一些资源的限制，加上程序的业务处理，所以，8G内存是很难达到100万连接的，当然，我们也可以通过增加内存的方式增加并发量。</p><h3 id="Tomcat依赖的JVM的配置"><a href="#Tomcat依赖的JVM的配置" class="headerlink" title="Tomcat依赖的JVM的配置"></a>Tomcat依赖的JVM的配置</h3><p>我们知道Tomcat是Java程序，运行在JVM上，因此我们还需要对JVM做优化，才能更好的提升Tomcat的性能，简单带大家了解一下JVM，如下图所示。</p><p><img src="https://mic-blob-bucket.oss-cn-beijing.aliyuncs.com/202110171350493.png" alt="image-20210623204411021"></p><p>在JVM中，内存划分为堆、程序计数器、本地方发栈、方法区（元空间）、虚拟机栈。</p><h4 id="堆空间说明"><a href="#堆空间说明" class="headerlink" title="堆空间说明"></a>堆空间说明</h4><p>其中，堆内存是JVM内存中最大的一块区域，几乎所有的对象和数组都会被分配到堆内存中，它被所有线程共享。 堆空间被划分为新生代和老年代，新生代进一步划分为Eden和Surivor区，如下图所示。</p><p><img src="https://mic-blob-bucket.oss-cn-beijing.aliyuncs.com/202110171350001.png" alt="image-20210623205840226"></p><p>新生代和老年代的比例是1：2，也就是新生代会占1/3的堆空间，老年代会占2/3的堆空间。 另外，在新生代中，空间占比为Eden:Surivor0:Surivor1=8:1:1 。 举个例子来说，如果eden区内存大小是40M，那么两个Survivor区分别是占5M，整个新生代就是50M，然后计算出老年代的内存大小是100M，也就是说堆空间的总内存大小是150M。</p><blockquote><p>可以通过 java -XX:PrintFlagsFinal -version查看默认参数</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">uintx InitialSurvivorRatio                      = 8</span><br><span class="line">uintx NewRatio                                  = 2</span><br></pre></td></tr></table></figure><p>InitialSurvivorRatio:  新生代Eden/Survivor空间的初始比例</p><p>NewRatio ： Old区/Young区的内存比例</p></blockquote><p>堆内存的具体工作原理是：</p><ul><li>绝大部分的对象被创建之后，会保存在Eden区，当Eden区满了的时候，就会触发YGC（Young GC），大部分对象会被回收掉，如果还有活着的对象，就拷贝到Survivor0，这时Eden区被清空。</li><li>如果后续再次触发YGC，活着的对象Eden+Survivor0中的对象拷贝到Survivor1区， 这时Eden和Survivor0都会被清空</li><li>接着再触发YGC，Eden+Survivor1中的对象会被拷贝到Survivor0区，一直这么循环，直到对象的年龄达到阈值，则放入到老年代。（之所以这么设计，是因为Eden区的大部分对象会被回收）</li><li>Survivor区装不下的对象会直接进入到老年代</li><li>老年代满了，会触发Full GC。</li></ul><blockquote><p>GC标记-清除算法 在执行过程中暂停其他线程??</p></blockquote><p><img src="https://mic-blob-bucket.oss-cn-beijing.aliyuncs.com/202110171350497.png" alt="image-20210623214030533"></p><h4 id="程序计数器"><a href="#程序计数器" class="headerlink" title="程序计数器"></a>程序计数器</h4><p>程序计数器是用来记录各个线程执行的字节码地址等，当线程发生上下文切换时，需要依靠这个来记住当前执行的位置，当下次恢复执行后要沿着上一次执行的位置继续执行。</p><h4 id="方法区"><a href="#方法区" class="headerlink" title="方法区"></a>方法区</h4><p>方法区是逻辑上的概念，在HotSpot虚拟机的1.8版本中，它的具体实现就是元空间。</p><p>方法区主要用来存放已经被虚拟机加载的类相关信息，包括类元信息、运行时常量池、字符串常量池，类信息又包括类的版本、字段、方法、接口和父类信息等。</p><p>方法区和堆空间类似，它是一个共享内存区域，所以方法区是属于线程共享的。</p><h4 id="本地方发栈和虚拟机栈"><a href="#本地方发栈和虚拟机栈" class="headerlink" title="本地方发栈和虚拟机栈"></a>本地方发栈和虚拟机栈</h4><p>Java虚拟机栈是线程私有的内存空间，当创建一个线程时，会在虚拟机中申请一个线程栈，用来保存方法的局部变量、操作数栈、动态链接方法等信息。每一个方法的调用都伴随这栈帧的入栈操作，当一个方法返回之后，就是栈帧的出栈操作。</p><p>本地方法栈和虚拟机栈类似，本地方法栈是用来管理本地方法的调用，也就是native方法。</p><h4 id="JVM内存应该怎么设置"><a href="#JVM内存应该怎么设置" class="headerlink" title="JVM内存应该怎么设置"></a>JVM内存应该怎么设置</h4><p>了解了上述基本信息之后，那么JVM中内存应该如何设置呢？有哪些参数来设置？</p><p>而在JVM中，要配置的几个核心参数无非是。</p><ul><li><p><code>-Xms</code>，Java堆内存大小</p></li><li><p><code>-Xmx</code>，Java最大堆内存大小</p></li><li><p><code>-Xmn</code>，Java堆内存中的新生代大小，扣除新生代剩下的就是老年代内存</p><p>新生代内存设置过小会频繁触发Minor GC，频繁触发GC会影响系统的稳定性</p></li><li><p><code>-XX:MetaspaceSize</code>，元空间大小， 128M</p></li><li><p><code>-XX:MaxMetaspaceSize</code>，最大云空间大小 （如果没有指定这两个参数，元空间会在运行时根据需要动态调整。）  256M</p><blockquote><p>一个新系统的元空间，基本上没办法有一个测算的方法，一般设置几百兆就够用，因为这里面主要存放一些类信息。</p></blockquote></li><li><p><code>-Xss</code>，线程栈内存大小，这个基本上不需要预估，设置512KB到1M就行，因为值越小，能够分配的线程数越多。</p></li></ul><p>JVM内存的大小，取决于机器的配置，比如一个2核4G的服务器，能够分配给JVM进程也就2G左右，因为机器本身也需要内存，而且机器上还运行了其他的进程也需要占内存。而这2G还得分配给栈内存、堆内存、元空间，那堆内存能够得到的也就1G左右，然后堆内存还要分新生代、老年代。</p><h3 id="Tomcat本身的配置"><a href="#Tomcat本身的配置" class="headerlink" title="Tomcat本身的配置"></a>Tomcat本身的配置</h3><blockquote><p><a href="http://tomcat.apache.org/tomcat-8.0-doc/config/http.html">http://tomcat.apache.org/tomcat-8.0-doc/config/http.html</a></p></blockquote><blockquote><p>The maximum number of request processing threads to be created by this <strong>Connector</strong>, which therefore determines the maximum number of simultaneous requests that can be handled. If not specified, this attribute is set to 200. If an executor is associated with this connector, this attribute is ignored as the connector will execute tasks using the executor rather than an internal thread pool. Note that if an executor is configured any value set for this attribute will be recorded correctly but it will be reported (e.g. via JMX) as <code>-1</code> to make clear that it is not used.</p></blockquote><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">server:</span></span><br><span class="line">  <span class="attr">tomcat:</span></span><br><span class="line">    <span class="attr">uri-encoding:</span> <span class="string">UTF-8</span></span><br><span class="line">    <span class="comment">#最大工作线程数，默认200, 4核8g内存，线程数经验值800</span></span><br><span class="line">    <span class="comment">#操作系统做线程之间的切换调度是有系统开销的，所以不是越多越好。</span></span><br><span class="line">    <span class="attr">max-threads:</span> <span class="number">1000</span></span><br><span class="line">    <span class="comment"># 等待队列长度，默认100，</span></span><br><span class="line">    <span class="attr">accept-count:</span> <span class="number">1000</span></span><br><span class="line">    <span class="attr">max-connections:</span> <span class="number">20000</span></span><br><span class="line">    <span class="comment"># 最小工作空闲线程数，默认10, 适当增大一些，以便应对突然增长的访问量</span></span><br><span class="line">    <span class="attr">min-spare-threads:</span> <span class="number">100</span></span><br></pre></td></tr></table></figure><ul><li><p><strong>accept-count:</strong> 最大等待数，当调用HTTP请求数达到tomcat的最大线程数时，还有新的HTTP请求到来，这时tomcat会将该请求放在等待队列中，这个acceptCount就是指能够接受的最大等待数，默认100。如果等待队列也被放满了，这个时候再来新的请求就会被tomcat拒绝（connection refused）</p></li><li><p><strong>maxThreads：</strong>最大线程数，每一次HTTP请求到达Web服务，tomcat都会创建一个线程来处理该请求，那么最大线程数决定了Web服务容器可以同时处理多少个请求。maxThreads默认200，肯定建议增加。但是，增加线程是有成本的，更多的线程，不仅仅会带来更多的线程上下文切换成本，而且意味着带来更多的内存消耗。JVM中默认情况下在创建新线程时会分配大小为1M的线程栈，所以，更多的线程异味着需要更多的内存。线程数的经验值为：1核2g内存为200，线程数经验值200；4核8g内存，线程数经验值800。</p></li><li><p><strong>maxConnections</strong>，最大连接数，这个参数是指在同一时间，tomcat能够接受的最大连接数。对于Java的阻塞式BIO，默认值是maxthreads的值；如果在BIO模式使用定制的Executor执行器，默认值将是执行器中maxthreads的值。对于Java 新的NIO模式，maxConnections 默认值是10000。对于windows上APR/native IO模式，maxConnections默认值为8192</p><p>如果设置为-1，则禁用maxconnections功能，表示不限制tomcat容器的连接数。<br><strong>maxConnections和accept-count的关系为：当连接数达到最大值maxConnections后，系统会继续接收连接，但不会超过acceptCount的值。</strong></p></li></ul><h3 id="1-3-4-应用带来的压力"><a href="#1-3-4-应用带来的压力" class="headerlink" title="1.3.4 应用带来的压力"></a>1.3.4 应用带来的压力</h3><p>前面我们分析过，NIOEndPoint接收到客户端请求连接后，会生成一个SocketProcessor任务给到线程池去处理，SocketProcessor中的run方法会调用HttpProcessor组件去解析应用层的协议，并生成Request对象。最后调用Adapter的Service方法，将请求传递到容器中。</p><p>容器主要负责内部的处理工作，也就是当前置的连接器通过Socket获取到信息之后，得到一个Servlet请求，而容器就是负责处理Servlet请求。</p><p>Tomcat使用Mapper组件将用户请求的URL定位到一个具体的Serlvet，然后Spring中的DispatcherServlet拦截到该Servlet请求后，基于Spring本身的Mapper映射定位到我们具体的Controller中。</p><p>到了Controller之后，对于我们的业务来说，才是一个请求真正的开始，Controller调用Service、Service调用dao，完成数据库操作之后，讲请求原路返回给到客户端，完成一次整体的会话。也就是说，Controller中的业务逻辑处理耗时，对于整个容器的并发来说也会受到影响。</p><p><img src="https://mic-blob-bucket.oss-cn-beijing.aliyuncs.com/202110171350088.png" alt="image-20210622151107514"></p><h2 id="服务器数量评估"><a href="#服务器数量评估" class="headerlink" title="服务器数量评估"></a>服务器数量评估</h2><p>通过上述分析，我们假设一个tomcat节点的QPS=500，如果要支撑到高峰时期的QPS=18000，那么需要40台服务器，这四台服务器需要通过Nginx软件负载均衡，进行请求分发，Nginx的性能很好，官方给的说明是Nginx处理静态文件的并发能够达到5W/s。另外Nginx由于不能单点，我们可以采用LVS对Nginx做负载均衡，LVS（Linux VirtualServer），它是采用IP负载均衡技术实现负载均衡。</p><p><img src="https://mic-blob-bucket.oss-cn-beijing.aliyuncs.com/202110171350886.png" alt="image-20210622220213652"></p><p>通过这样的一组架构，我们当前服务端是能够同时承接QPS=18000，但是还不够，再回到前面我们说的两个公式。</p><ul><li><p>QPS=并发量/平均响应时间</p></li><li><p>并发量=QPS*平均响应时间</p></li></ul><p>假设我们的RT是3s，那么意味着服务器端的并发数=18000*3=54000，也就是同时有54000个连接打到服务器端，所以服务端需要同时支持的连接数为54000，这个我们在前文说过如何进行配置。如果RT越大，那么意味着堆积的链接越多，而这些连接会占用内存资源/CPU资源等，容易造成系统崩溃的现象。同时，当链接数超过阈值时，后续的请求无法进来，用户会得到一个请求超时的结果，这显然不是我们所希望看到的，所以我们必须要缩短RT的值。</p>]]></content>
      
      
      <categories>
          
          <category> 架构设计 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 高并发 </tag>
            
            <tag> 架构设计 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/posts/1243066710/"/>
      <url>/posts/1243066710/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>

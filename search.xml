<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>工作3年的Java程序员，轻松拿到阿里P6Offer，只因为他搞明白了Redis这几个问题！！</title>
      <link href="/posts/2781994300/"/>
      <url>/posts/2781994300/</url>
      
        <content type="html"><![CDATA[<h1 id="Redis中的多路复用模型"><a href="#Redis中的多路复用模型" class="headerlink" title="Redis中的多路复用模型"></a>Redis中的多路复用模型</h1><p>Redis6用到了多线程？那多线程应用在哪些地方，引入多线程后，又改如何保证线程安全性呢？<br>同时，如何在性能和线程安全性方面做好平衡？</p><h2 id="关于Redis的单线程模型"><a href="#关于Redis的单线程模型" class="headerlink" title="关于Redis的单线程模型"></a>关于Redis的单线程模型</h2><p>在Redis6.0之前，我们一直说Redis是单线程，所以并不会存在线程安全问题，而这个单线程，实际上就是在做数据IO处理中，是用的主线程来串行执行，如图4-7所示。</p><p>Redis基于Reactor模式设计开发了自己的一套高效事件处理模型，这个事件处理模型对应的就是Redis中的文件事件处理器，这个文件事件处理器是单线程运行的，这也是为什么我们一直强调Redis是线程安全的。</p><p>既然Redis是基于Reactor模型实现，那它必然用了I/O多路复用机制来监听多个客户端连接，然后把感兴趣的事件（READ/ACCEPT/CLOSE/WRITE）注册到多路复用器中。</p><p>文件事件处理器中使用I/O多路复用模型同时监听多个客户端连接，并且根据当前连接执行的任务类型关联不同的事件处理器（连接应答处理器、命令请求处理器、命令回复处理器）来处理这些事件。</p><p>这样设计的好处：</p><ul><li>文件事件处理器实现了高性能的网络IO通信模型</li><li>通过单线程的方式执行指令，避免同步机制的性能开销、避免过多的上下文切换、整体实现比较简单，不需要考虑多线程场景中的各种数据结构的线程安全问题。</li></ul><p><img src="https://mic-blob-bucket.oss-cn-beijing.aliyuncs.com/202110151354544.png" alt="image-20210708232804607"></p><center>图4-7</center><p>其实严格意义上来说，在Redis4.x版本就支持了多线程，只是，<strong>负责客户端请求的IO处理使用的是单线程</strong>。但是针对那些非常耗时的命令，Redis4.x提供了异步化的指令来处理，避免因为IO时间过长影响到客户端请求IO处理的线程。比如在 Redis v4.0 之后增加了一些的非阻塞命令如 <code>UNLINK</code>（del命令的异步版本）、<code>FLUSHALL ASYNC</code>、<code>FLUSHDB ASYNC</code>。</p><h2 id="Redis6-0之后的多线程？"><a href="#Redis6-0之后的多线程？" class="headerlink" title="Redis6.0之后的多线程？"></a>Redis6.0之后的多线程？</h2><p>在Redis6.0中引入了多线程，可能很多同学会误以为redis原本的单线程数据IO变成了多线程IO，那作者不就是在打自己的脸吗？</p><blockquote><p>对于Redis来说，CPU通常不是瓶颈，因为大多数请求不是属于CPU密集型，而是I/O密集型。而在Redis中除了数据的持久化方案之外，它是完全的纯内存操作，因此执行速度是非常快的，所以数据的IO并不是Redis的性能瓶颈，Redis真正的性能瓶颈是在网络I/O，也就是客户端和服务端之间的网络传输延迟，所以Redis选择了单线程的IO多路复用来实现它的核心网络模型。</p></blockquote><p>前面我们说过，单线程设计对于Redis来说有很多好处。</p><ul><li>避免过多的上上下文切换开销</li><li>避免同步机制的开销，涉及到数据同步和事务操作时，避免多线程影响所以必然需要加同步机制保证线程安全性。但是加锁同时也会影响到程序的执行性能。 </li><li>维护简单，引入多线程之后，不管是对数据结构的设计，还是在程序代码的维护上，都会变得很复杂。</li></ul><p>所以既然Redis的数据I/O不是瓶颈，同时单线程又有这么多好处，那Redis自然就采用单线程了。既然是这样，那么Redis 6.0引入多线程，一定不是优化数据IO性能，那么我们先来分析一下Redis性能瓶颈主要体现在哪些方面，无非就是三个方面。</p><ul><li>网络IO</li><li>CPU核心数</li><li>内存</li></ul><p>由于CPU核心数并不是redis的瓶颈，所以影响Redis性能的因素只有网络IO和内存，而内存属于硬件范畴，比如采用容量更大、吞吐量更高的内存进行优化就行，因此也不是属于Redis可优化的空间，所以最终我们发现Redis的性能瓶颈还是在网络IO上。</p><p>而在Redis6.0之前，使用的是单线程Reactor模型，单线程模型是指对于客户端的请求，主线程需要负责对这个请求的完整IO过程进行处理，如图4-8所示，从socket中读取数据和往socket中写数据都是比较耗时的网络IO操作，解析请求和内存交互耗时可能远小于这个网络IO操作。</p><p><img src="https://mic-blob-bucket.oss-cn-beijing.aliyuncs.com/202110151354708.png" alt="image-20210710153215329"></p><center>图4-8</center><p>按照前面我们对多Reactor多线程的理解，那我们能不能改成主从多Reactor多线程模型呢？主Reactor负责接收客户端连接，然后分发给多个Reactor进行网络IO操作。很显然，这样做就会导致Redis编程了一个多线程模型，这对Redis的影响较大，因为多线程带来的线程安全问题和底层复杂的数据结构的操作都非常棘手，所以Redis 6.0并没有这么做。</p><p>Redis 6.0中将处理过程中最耗时的Socket读取、请求解析、单独用一个线程来处理，剩下的命令执行操作仍然由单线程来完成和内存的数据交互，这样一来，网络IO操作就变成了多线程了，但是核心部分仍然是线程安全的，如图4-9所示。</p><p><img src="https://mic-blob-bucket.oss-cn-beijing.aliyuncs.com/202110151354391.png" alt="image-20210710154600353"></p><center>图4-9</center><p>为什么说Redis6.0是一个特殊的多线程，原因就在这里，Redis主要针对网络IO这块引入了多线程的方式来提升了网络IO性能，但是真正执行命令的操作仍然是由主线程来完成。因此，总的来说，我们仍然可以说Redis是单线程模型。</p><h2 id="Redis-6-0如何开启多线程"><a href="#Redis-6-0如何开启多线程" class="headerlink" title="Redis 6.0如何开启多线程"></a>Redis 6.0如何开启多线程</h2><p>Redis 6.0默认多线程是禁止的，也就是仍然只是使用主线程来完成网络IO，如果需要开启，则修改redis.conf配置文件中的如下属性</p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 默认是关闭，设置为yes打开</span></span><br><span class="line"><span class="meta">io-threads-do-reads</span> <span class="string">no</span></span><br><span class="line"><span class="comment">#默认线程数量是4，官方建议是4核机器上设置为2~3个，8核机器上设置6个</span></span><br><span class="line"><span class="meta">io-threads</span> <span class="string">4</span></span><br></pre></td></tr></table></figure><h2 id="引入多线程之后的性能提升"><a href="#引入多线程之后的性能提升" class="headerlink" title="引入多线程之后的性能提升"></a>引入多线程之后的性能提升</h2><p>图4-20是美团技术团队使用阿里云服务器压测GET/SET命令在4个线程IO时性能上的对比结果，可以明显的看到，Redis 在使用多线程模式之后性能大幅提升，达到了一倍。</p><ul><li>Redis Server 阿里云 Ubuntu 18.04  ，  8CPU 2.5GHZ，8G内存，主机型号： ecs.ic5.2xlarge</li><li>Redis Benchmark client: 阿里云 Unbuntu 18.04 , 8CPU  2.5GHZ，8G内存，主机型号：ecs.ic5.2xlarge</li></ul><p><img src="https://mic-blob-bucket.oss-cn-beijing.aliyuncs.com/202110151354177.png" alt="preview"></p><p><img src="https://mic-blob-bucket.oss-cn-beijing.aliyuncs.com/202110151354660.png" alt="preview"></p><center>图4-20</center><h1 id="内存回收策略"><a href="#内存回收策略" class="headerlink" title="内存回收策略"></a>内存回收策略</h1><p>很多同学了解了Redis的好处之后，于是把任何数据都往Redis中放，如果使用不合理很容易导致数据超过Redis的内存，这种情况会出现什么问题呢？</p><ul><li>Redis中有很多无效的缓存，这些缓存数据会降低数据IO的性能，因为不同的数据类型时间复杂度算法不同，数据越多可能会造成性能下降</li><li>随着系统的运行，redis的数据越来越多，会导致物理内存不足。通过使用虚拟内存（VM），将很少访问的数据交换到磁盘上，腾出内存空间的方法来解决物理内存不足的情况。虽然能够解决物理内存不足导致的问题，但是由于这部分数据是存储在磁盘上，如果在高并发场景中，频繁访问虚拟内存空间会严重降低系统性能。</li></ul><p>所以遇到这类问题的时候，我们一般有几种方法。</p><ul><li>对每个存储到redis中的key设置过期时间，这个根据实际业务场景来决定。否则，再大的内存都会虽则系统运行被消耗完。</li><li>增加内存</li><li>使用内存淘汰策略。</li></ul><h2 id="设置Redis能够使用的最大内存"><a href="#设置Redis能够使用的最大内存" class="headerlink" title="设置Redis能够使用的最大内存"></a>设置Redis能够使用的最大内存</h2><p>在实际生产环境中，服务器不仅仅只有Redis，为了避免Redis内存使用过多对其他程序造成影响，我们一般会设置最大内存。</p><p>Redis默认的最大内存<code>maxmemory=0</code>，表示不限制Redis内存的使用。我们可以修改<code>redis.conf</code>文件，设置Redis最大使用的内存。</p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 单位为byte</span></span><br><span class="line"><span class="attr">maxmemory</span> <span class="string">&lt;bytes&gt;  2147483648（2G）</span></span><br></pre></td></tr></table></figure><p>如何查看当前Redis最大内存设置呢，进入到Redis-Cli控制台，输入下面这个命令。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">config get maxmemory</span><br></pre></td></tr></table></figure><p>当Redis中存储的内存超过maxmemory时，会怎么样呢？下面我们做一个实验</p><ul><li><p>在redis-cli控制台输入下面这个命令，把最大内存设置为1个字节。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">config set maxmemory 1</span><br></pre></td></tr></table></figure></li><li><p>通过下面的命令存储一个string类型的数据</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">set name mic</span><br></pre></td></tr></table></figure></li><li><p>此时，控制台会得到下面这个错误信息</p></li></ul><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">(error) OOM command not allowed when used memory &gt; &#x27;maxmemory&#x27;.</span><br></pre></td></tr></table></figure><h2 id="使用内存淘汰策略释放内存"><a href="#使用内存淘汰策略释放内存" class="headerlink" title="使用内存淘汰策略释放内存"></a>使用内存淘汰策略释放内存</h2><p>设置了maxmemory的选项，redis内存使用达到上限。可以通过设置LRU算法来删除部分key，释放空间。默认是按照过期时间的，如果set时候没有加上过期时间就会导致数据写满maxmemory。</p><p>Redis中提供了一种内存淘汰策略，当内存不足时，Redis会根据相应的淘汰规则对key数据进行淘汰。 Redis一共提供了8种淘汰策略，默认的策略为<strong>noeviction</strong>，当内存使用达到阈值的时候，</p><p>所有引起申请内存的命令会报错。</p><ul><li><strong>volatile-lru</strong>，针对设置了过期时间的key，使用lru算法进行淘汰。</li><li><strong>allkeys-lru</strong>，针对所有key使用lru算法进行淘汰。</li><li><strong>volatile-lfu</strong>，针对设置了过期时间的key，使用lfu算法进行淘汰。</li><li><strong>allkeys-lfu</strong>，针对所有key使用lfu算法进行淘汰。</li><li><strong>volatile-random</strong>，从所有设置了过期时间的key中使用随机淘汰的方式进行淘汰。</li><li><strong>allkeys-random</strong>，针对所有的key使用随机淘汰机制进行淘汰。</li><li><strong>volatile-ttl</strong>，删除生存时间最近的一个键。</li><li><strong>noeviction</strong>，不删除键，值返回错误。</li></ul><p>前缀为volatile-和allkeys-的区别在于二者选择要清除的键时的字典不同，volatile-前缀的策略代表从redisDb中的expire字典中选择键进行清除；allkeys-开头的策略代表从dict字典中选择键进行清除。</p><p>内存淘汰算法的具体工作原理是：</p><ul><li>客户端执行一条新命令，导致数据库需要增加数据（比如set key value）</li><li>Redis会检查内存使用，如果内存使用超过 maxmemory，就会按照置换策略删除一些 key</li><li>新的命令执行成功</li></ul><h3 id="了解并手写LRU算法"><a href="#了解并手写LRU算法" class="headerlink" title="了解并手写LRU算法"></a>了解并手写LRU算法</h3><p>LRU是Least Recently Used的缩写，也就是表示最近很少使用，也可以理解成最久没有使用。也就是说当内存不够的时候，每次添加一条数据，都需要抛弃一条最久时间没有使用的旧数据。</p><p>标准的LRU算法为了降低查找和删除元素的时间复杂度，一般采用Hash表和双向链表结合的数据结构，hash表可以赋予链表快速查找到某个key是否存在链表中，同时可以快速删除、添加节点，如图4-21所示。</p><blockquote><p>双向链表的查找时间复杂度是O(n)，删除和插入是O(1)，借助HashMap结构，可以使得查找的时间复杂度变成O(1)</p></blockquote><p>Hash表用来查询在链表中的数据位置，链表负责数据的插入，当新数据插入到链表头部时有两种情况。</p><ul><li>链表满了，把链表尾部的数据丢弃掉，新加入的缓存直接加入到链表头中。</li><li>当链表中的某个缓存被命中时，直接把数据移到链表头部，原本在头节点的缓存就向链表尾部移动</li></ul><p>这样，经过多次Cache操作之后，最近被命中的缓存，都会存在链表头部的方向，没有命中的，都会在链表尾部方向，当需要替换内容时，由于链表尾部是最少被命中的，我们只需要淘汰链表尾部的数据即可。</p><p><img src="https://mic-blob-bucket.oss-cn-beijing.aliyuncs.com/202110151354493.png" alt="image-20210710205446429"></p><center>图4-21</center><p>下面我们通过一段代码实现一个简单的LRU算法，加深大家对于LRU算法的理解。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LRUCache</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Node head;</span><br><span class="line">    <span class="keyword">private</span> Node tail;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> HashMap&lt;String,Node&gt; nodeHashMap;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> capacity;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">LRUCache</span><span class="params">(<span class="keyword">int</span> capacity)</span></span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.capacity=capacity;</span><br><span class="line">        nodeHashMap=<span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">        head=<span class="keyword">new</span> Node();</span><br><span class="line">        tail=<span class="keyword">new</span> Node();</span><br><span class="line">        head.next=tail;</span><br><span class="line">        tail.prev=head;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">removeNode</span><span class="params">(Node node)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(node==tail)&#123;</span><br><span class="line">            tail=tail.prev;</span><br><span class="line">            tail.next=<span class="keyword">null</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span>(node==head)&#123;</span><br><span class="line">            head=head.next;</span><br><span class="line">            head.prev=<span class="keyword">null</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">            node.prev.next=node.next;</span><br><span class="line">            node.next.prev=node.prev;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">addNodeToHead</span><span class="params">(Node node)</span></span>&#123;</span><br><span class="line">        node.next=head.next;</span><br><span class="line">        head.next.prev=node;</span><br><span class="line">        node.prev=head;</span><br><span class="line">        head.next=node;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">addNodeToTail</span><span class="params">(Node node)</span></span>&#123;</span><br><span class="line">        node.prev=tail.prev;</span><br><span class="line">        node.prev.next=node;</span><br><span class="line">        node.next=tail;</span><br><span class="line">        tail.prev=node;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//当链表中的某个缓存被命中时，直接把数据移到链表头部，原本在头节点的缓存就向链表尾部移动</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">moveNodeToHead</span><span class="params">(Node node)</span></span>&#123;</span><br><span class="line"></span><br><span class="line">        removeNode(node);</span><br><span class="line">        addNodeToHead(node);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">get</span><span class="params">(String key)</span></span>&#123;</span><br><span class="line">        Node node=nodeHashMap.get(key);</span><br><span class="line">        <span class="keyword">if</span>(node==<span class="keyword">null</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//刷新当前节点的位置</span></span><br><span class="line">        moveNodeToHead(node);</span><br><span class="line">        <span class="comment">//返回value值</span></span><br><span class="line">        <span class="keyword">return</span> node.value;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">put</span><span class="params">(String key,String value)</span></span>&#123;</span><br><span class="line">        Node node=nodeHashMap.get(key);</span><br><span class="line">        <span class="keyword">if</span>(node==<span class="keyword">null</span>)&#123; <span class="comment">//不存在</span></span><br><span class="line">            <span class="comment">//如果当前存储的数据量达到了阈值，则需要淘汰掉访问较少的数据</span></span><br><span class="line">            <span class="keyword">if</span>(nodeHashMap.size()&gt;=capacity)&#123;</span><br><span class="line">                removeNode(tail); <span class="comment">//移除尾部节点</span></span><br><span class="line">                nodeHashMap.remove(tail.key);</span><br><span class="line">            &#125;</span><br><span class="line">            node=<span class="keyword">new</span> Node(key,value);</span><br><span class="line">            nodeHashMap.put(key,node);</span><br><span class="line">            addNodeToTail(node);</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            node.value=value;</span><br><span class="line">            <span class="comment">//刷新当前节点的位置</span></span><br><span class="line">            moveNodeToHead(node);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        LRUCache lruCache=<span class="keyword">new</span> LRUCache(<span class="number">3</span>);</span><br><span class="line">        lruCache.put(<span class="string">&quot;1&quot;</span>,<span class="string">&quot;1&quot;</span>);</span><br><span class="line">        lruCache.put(<span class="string">&quot;2&quot;</span>,<span class="string">&quot;2&quot;</span>);</span><br><span class="line">        lruCache.put(<span class="string">&quot;3&quot;</span>,<span class="string">&quot;3&quot;</span>);</span><br><span class="line"><span class="comment">//        lruCache.get(&quot;3&quot;); // 增加一个访问次数之后，被清理的元素就会发生变化</span></span><br><span class="line">        System.out.println(lruCache.nodeHashMap);</span><br><span class="line">        lruCache.put(<span class="string">&quot;4&quot;</span>,<span class="string">&quot;4&quot;</span>);</span><br><span class="line">        System.out.println(lruCache.nodeHashMap);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span></span>&#123;</span><br><span class="line">    <span class="comment">//双向链表中的节点类，存储key是因为我们在双向链表删除表尾的值时，只是返回了一个节点，</span></span><br><span class="line">    <span class="comment">//所以这个节点要包括key值，这样我们的哈希表才可以删除对应key值的映射</span></span><br><span class="line">    <span class="keyword">public</span> String key;</span><br><span class="line">    <span class="keyword">public</span> String value;</span><br><span class="line">    Node prev;</span><br><span class="line">    Node next;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Node</span><span class="params">()</span></span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Node</span><span class="params">(String key, String value)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.key = key;</span><br><span class="line">        <span class="keyword">this</span>.value = value;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Redis中的LRU算法"><a href="#Redis中的LRU算法" class="headerlink" title="Redis中的LRU算法"></a>Redis中的LRU算法</h3><p>实际上，Redis使用的LRU算法其实是一种不可靠的LRU算法，它实际淘汰的键并不一定是真正最少使用的数据，它的工作机制是：</p><ul><li>随机采集淘汰的key，每次随机选出5个key</li><li>然后淘汰这5个key中最少使用的key</li></ul><p>这5个key是默认的个数，具体的数值可以在redis.conf中配置</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">maxmemory-samples 5</span><br></pre></td></tr></table></figure><p>当近似LRU算法取值越大的时候就会越接近真实的LRU算法，因为取值越大获取的数据越完整，淘汰中的数据就更加接近最少使用的数据。这里其实涉及一个权衡问题，</p><p>如果需要在所有的数据中搜索最符合条件的数据，那么一定会增加系统的开销，Redis是单线程的，所以耗时的操作会谨慎一些。</p><p>为了在一定成本内实现相对的LRU，早期的Redis版本是基于采样的LRU，也就是放弃了从所有数据中搜索解改为采样空间搜索最优解。Redis3.0版本之后，Redis作者对于基于采样的LRU进行了一些优化：</p><ul><li>Redis中维护一个大小为16的候选池，当第一次随机选取采用数据时，会把数据放入到候选池中，并且候选池中的数据会更具时间进行排序。</li><li>当第二次以后选取数据时，只有小于候选池内最小时间的才会被放进候选池。</li><li>当候选池的数据满了之后，那么时间最大的key就会被挤出候选池。当执行淘汰时，直接从候选池中选取最近访问时间小的key进行淘汰。</li></ul><p>如图4-22所示，首先从目标字典中采集出maxmemory-samples个键，缓存在一个samples数组中，然后从samples数组中一个个取出来，和回收池中以后的键进行键的空闲时间，从而更新回收池。</p><p>在更新过程中，首先利用遍历找到的每个键的实际插入位置x，然后根据不同情况进行处理。</p><ul><li>回收池满了，并且当前插入的key的空闲时间最小（也就是回收池中的所有key都比当前插入的key的空闲时间都要大），则不作任何操作。</li><li>回收池未满，并且插入的位置x没有键，则直接插入即可</li><li>回收池未满，且插入的位置x原本已经存在要淘汰的键，则把第x个以后的元素都往后挪一个位置，然后再执行插入操作。</li><li>回收池满了，将当前第x个以前的元素往前挪一个位置（实际就是淘汰了），然后执行插入操作。</li></ul><p><img src="https://mic-blob-bucket.oss-cn-beijing.aliyuncs.com/202110151354542.png" alt="image-20210710203108453"></p><center>图4-22</center><p>这样做的目的是能够选出最真实的最少被访问的key，能够正确不常使用的key。因为在Redis3.0之前是随机选取样本，这样的方式很有可能不是真正意义上的最少访问的key。</p><p>LRU算法有一个弊端，加入一个key值访问频率很低，但是最近一次被访问到了，那LRU会认为它是热点数据，不会被淘汰。同样，</p><p>经常被访问的数据，最近一段时间没有被访问，这样会导致这些数据被淘汰掉，导致误判而淘汰掉热点数据，于是在Redis 4.0中，新加了一种LFU算法。</p><h3 id="LFU算法"><a href="#LFU算法" class="headerlink" title="LFU算法"></a>LFU算法</h3><p>LFU（Least Frequently Used），表示最近最少使用，它和key的使用次数有关，其思想是：根据key最近被访问的频率进行淘汰，比较少访问的key优先淘汰，反之则保留。</p><p>LRU的原理是使用计数器来对key进行排序，每次key被访问时，计数器会增大，当计数器越大，意味着当前key的访问越频繁，也就是意味着它是热点数据。 它很好的解决了LRU算法的缺陷：<strong>一个很久没有被访问的key，偶尔被访问一次，导致被误认为是热点数据的问题。</strong></p><p>LFU的实现原理如图4-23所示，LFU维护了两个链表，横向组成的链表用来存储访问频率，每个访问频率的节点下存储另外一个具有相同访问频率的缓存数据。具体的工作原理是：</p><ul><li>当添加元素时，找到相同访问频次的节点，然后添加到该节点的数据链表的头部。如果该数据链表满了，则移除链表尾部的节点</li><li>当获取元素或者修改元素是，都会增加对应key的访问频次，并把当前节点移动到下一个频次节点。</li></ul><blockquote><p>添加元素时，访问频率默认为1，随着访问次数的增加，频率不断递增。而当前被访问的元素也会随着频率增加进行移动。</p></blockquote><p><img src="https://mic-blob-bucket.oss-cn-beijing.aliyuncs.com/202110151354188.png" alt="image-20210710213258901"></p><center>图4-23</center><h1 id="持久化机制的实现及原理"><a href="#持久化机制的实现及原理" class="headerlink" title="持久化机制的实现及原理"></a>持久化机制的实现及原理</h1><p>Redis的强劲性能很大程度上是由于它所有的数据都存储在内存中，当然如果redis重启或者服务器故障导致redis重启，所有存储在内存中的数据就会丢失。但是在某些情况下，我们希望Redis在重启后能够保证数据不会丢失。</p><ol><li><p>将redis作为nosql数据库使用。</p></li><li><p>将Redis作为高效缓存服务器，缓存被击穿后对后端数据库层面的瞬时压力是特别大的，所有缓存同时失效可能会导致雪崩。</p></li></ol><p>这时我们希望Redis能将数据从内存中以某种形式同步到硬盘上，使得重启后可以根据硬盘中的记录来恢复数据。</p><p>Redis支持两种方式的持久化，一种是RDB方式、另一种是AOF（append-only-file）方式，两种持久化方式可以单独使用其中一种，也可以将这两种方式结合使用。</p><ul><li><strong>RDB</strong>：根据指定的规则“<strong>定时</strong>”将内存中的数据存储在硬盘上，</li><li><strong>AOF</strong>：每次执行命令后将命令本身记录下来。</li></ul><h3 id="4-3-1-RDB模式"><a href="#4-3-1-RDB模式" class="headerlink" title="4.3.1 RDB模式"></a>4.3.1 RDB模式</h3><p>RDB的持久化方式是通过快照（snapshotting）完成的，它是Redis默认的持久化方式，配置如下。</p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="comment"># save 3600 1</span></span><br><span class="line"><span class="comment"># save 300 100</span></span><br><span class="line"><span class="comment"># save 60 10000</span></span><br></pre></td></tr></table></figure><p>Redis允许用户自定义快照条件，当符合快照条件时，Redis会自动执行快照操作。快照的条件可以由用户在配置文件中配置。配置格式如下</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">save &lt;seconds&gt; &lt;changes&gt;</span><br></pre></td></tr></table></figure><p>第一个参数是时间窗口，第二个是键的个数，也就是说，在第一个时间参数配置范围内被更改的键的个数大于后面的changes时，即符合快照条件。当触发条件时，Redis会自动将内存中的数据生成一份副本并存储在磁盘上，这个过程称之为“快照”，除了上述规则之外，还有以下几种方式生成快照。</p><ol><li>根据配置规则进行自动快照</li><li>用户执行SAVE或者GBSAVE命令</li><li>执行FLUSHALL命令</li><li>执行复制(replication)时</li></ol><h3 id="根据配置规则进行自动快照"><a href="#根据配置规则进行自动快照" class="headerlink" title="根据配置规则进行自动快照"></a>根据配置规则进行自动快照</h3><ul><li>修改redis.conf文件，表示5秒内，有一个key发生变化，就会生成rdb文件。</li></ul><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">save 5 1                # 表示3600s以内至少发生1个key变化（新增、修改、删除），则重写rdb文件</span><br><span class="line">save 300 100</span><br><span class="line">save 60 10000</span><br></pre></td></tr></table></figure><ul><li><p>修改文件存储路径</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">dir /data/program/redis/bin</span><br></pre></td></tr></table></figure></li><li><p>其他参数配置说明</p><table><thead><tr><th><strong>参数</strong></th><th><strong>说明</strong></th></tr></thead><tbody><tr><td>dir</td><td>rdb文件默认在启动目录下（相对路径） <code>config get dir</code> 获取</td></tr><tr><td>dbfilename</td><td>文件名称</td></tr><tr><td>rdbcompression</td><td>开启压缩可以节省存储空间，但是会消耗一些CPU的计算时间，默认开启</td></tr><tr><td>rdbchecksum</td><td>使用CRC64算法来进行数据校验，但是这样做会增加大约10%的性能消耗，如果希望获取到最大的性能提升，可以关闭此功能。</td></tr></tbody></table></li></ul><p><strong>如果需要关闭RDB的持久化机制，可以参考如下配置，开启<code>save</code>，并注释其他规则即可</strong></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">save &quot;&quot;</span><br><span class="line"><span class="meta">#</span><span class="bash">save 900 1</span></span><br><span class="line"><span class="meta">#</span><span class="bash">save 300 10</span></span><br><span class="line"><span class="meta">#</span><span class="bash">save 60 10000</span></span><br></pre></td></tr></table></figure><h3 id="用户执行SAVE或者GBSAVE命令"><a href="#用户执行SAVE或者GBSAVE命令" class="headerlink" title="用户执行SAVE或者GBSAVE命令"></a>用户执行SAVE或者GBSAVE命令</h3><p>除了让Redis自动进行快照以外，当我们对服务进行重启或者服务器迁移我们需要人工去干预备份。redis提供了两条命令来完成这个任务</p><ol><li><p><strong>save命令</strong></p><p>如图4-24所示，当执行save命令时，Redis同步做快照操作，在快照执行过程中会阻塞所有来自客户端的请求。当redis内存中的数据较多时，通过该命令将导致Redis较长时间的不响应。所以不建议在生产环境上使用这个命令，而是推荐使用bgsave命令</p><p><img src="https://mic-blob-bucket.oss-cn-beijing.aliyuncs.com/202110151355068.png" alt="image-20210712184050955"></p><center>图4-24</center></li><li><p><strong>bgsave命令</strong></p><p>如图4-25所示，bgsave命令可以在后台异步地进行快照操作，快照的同时服务器还可以继续响应来自客户端的请求。执行BGSAVE后，Redis会立即返回ok表示开始执行快照操作，在redis-cli终端，通过下面这个命令可以获取最近一次成功执行快照的时间（以 UNIX 时间戳格式表示）。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">LASTSAVE</span><br></pre></td></tr></table></figure></li></ol><p>1：redis使用fork函数复制一份当前进程的副本(子进程)</p><p>2：父进程继续接收并处理客户端发来的命令，而子进程开始将内存中的数据写入硬盘中的临时文件</p><p>3：当子进程写入完所有数据后会用该临时文件替换旧的RDB文件，至此，一次快照操作完成。 </p><blockquote><p>注意：redis在进行快照的过程中不会修改RDB文件，只有快照结束后才会将旧的文件替换成新的，也就是说任何时候RDB文件都是完整的。 这就使得我们可以通过定时备份RDB文件来实现redis数据库的备份， RDB文件是经过压缩的二进制文件，占用的空间会小于内存中的数据，更加利于传输。</p><p><strong>bgsave是异步执行快照的，bgsave写入的数据就是for进程时redis的数据状态，一旦完成fork，后续执行的新的客户端命令对数据产生的变更都不会反应到本次快照</strong></p></blockquote><p>Redis启动后会读取RDB快照文件，并将数据从硬盘载入到内存。根据数据量大小以及服务器性能不同，这个载入的时间也不同。</p><p><img src="https://mic-blob-bucket.oss-cn-beijing.aliyuncs.com/202110151355770.png" alt="image-20210712183559812"></p><center>图4-25</center><h3 id="执行FLUSHALL命令"><a href="#执行FLUSHALL命令" class="headerlink" title="执行FLUSHALL命令"></a>执行FLUSHALL命令</h3><p>该命令在前面讲过，会清除redis在内存中的所有数据。执行该命令后，只要redis中配置的快照规则不为空，也就是save 的规则存在。redis就会执行一次快照操作。不管规则是什么样的都会执行。如果没有定义快照规则，就不会执行快照操作。</p><h3 id="执行复制-replication-时"><a href="#执行复制-replication-时" class="headerlink" title="执行复制(replication)时"></a>执行复制(replication)时</h3><p>该操作主要是在主从模式下，redis会在复制初始化时进行自动快照。这个会在后面讲到；</p><p>这里只需要了解当执行复制操作时，即时没有定义自动快照规则，并且没有手动执行过快照操作，它仍然会生成RDB快照文件。</p><h3 id="RDB数据恢复演示"><a href="#RDB数据恢复演示" class="headerlink" title="RDB数据恢复演示"></a>RDB数据恢复演示</h3><ul><li>准备初始数据</li></ul><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">redis&gt;</span><span class="bash"> <span class="built_in">set</span> k1 1</span></span><br><span class="line"><span class="meta">redis&gt;</span><span class="bash"> <span class="built_in">set</span> k2 2</span></span><br><span class="line"><span class="meta">redis&gt;</span><span class="bash"> <span class="built_in">set</span> k3 3</span></span><br><span class="line"><span class="meta">redis&gt;</span><span class="bash"> <span class="built_in">set</span> k4 4</span></span><br><span class="line"><span class="meta">redis&gt;</span><span class="bash"> <span class="built_in">set</span> k5 5</span></span><br></pre></td></tr></table></figure><ul><li><p>通过shutdown命令关闭触发save</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">redis&gt;</span><span class="bash"> shutdown</span></span><br></pre></td></tr></table></figure></li><li><p>备份dump.rdb文件(用来后续恢复)</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cp dump.rdb dump.rdb.bak</span><br></pre></td></tr></table></figure></li><li><p>接着再启动redis-server(systemctl restart redis_6379)，通过keys命令查看，发现数据还在</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">keys *</span><br></pre></td></tr></table></figure></li></ul><blockquote><p>模拟数据丢失</p></blockquote><ul><li><p>执行flushall</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">redis&gt;</span><span class="bash"> flushall</span></span><br></pre></td></tr></table></figure></li><li><p>shutdown(重新生成没有数据的快照，用来模拟后续的数据恢复)</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">redis&gt;</span><span class="bash"> shutdown</span></span><br></pre></td></tr></table></figure></li><li><p>再次启动redis, 通过keys 命令查看，此时rdb中没有任何数据。</p></li><li><p>恢复之前备份的rdb文件（之前保存了数据的rdb快照）</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mv dump.rdb.bak dump.rdb</span><br></pre></td></tr></table></figure></li><li><p>再次重启redis，可以看到之前快照保存的数据</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">keys *</span><br></pre></td></tr></table></figure></li></ul><h3 id="RDB文件的优势和劣势"><a href="#RDB文件的优势和劣势" class="headerlink" title="RDB文件的优势和劣势"></a>RDB文件的优势和劣势</h3><p><strong>一、优势</strong></p><p>　　1.RDB是一个非常紧凑(compact)的文件，它保存了redis 在某个时间点上的数据集，这种文件非常适合用于进行备份和灾难恢复。</p><p>　　2.生成RDB文件的时候，redis主进程会fork()一个子进程来处理所有保存工作，主进程不需要进行任何磁盘IO操作。</p><p>　　3.RDB 在恢复大数据集时的速度比AOF的恢复速度要快。</p><p><strong>二、劣势</strong></p><ul><li><p>1、RDB方式数据没办法做到实时持久化/秒级持久化。因为bgsave每次运行都要执行fork操作创建子进程，频繁执行成本过高</p></li><li><p>2、在一定间隔时间做一次备份，所以如果redis意外down掉的话，就会丢失最后一次快照之后的所有修改（数据有丢失）。</p></li></ul><p><strong>如果数据相对来说比较重要，希望将损失降到最小，则可以使用AOF方式进行持久化。</strong></p><h3 id="4-3-2-AOF模式"><a href="#4-3-2-AOF模式" class="headerlink" title="4.3.2 AOF模式"></a>4.3.2 AOF模式</h3><p>AOF(Append Only File)：Redis 默认不开启。AOF采用日志的形式来记录每个写操作，并<strong>追加</strong>到文件中。开启后，执行更改Redis数据的命令时，就会把命令写入到AOF文件中。</p><p>Redis 重启时会根据日志文件的内容把写指令从前到后执行一次以完成数据的恢复工作。</p><h3 id="AOF配置开关"><a href="#AOF配置开关" class="headerlink" title="AOF配置开关"></a>AOF配置开关</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 开关</span></span><br><span class="line">appendonly no  /yes</span><br><span class="line"><span class="meta">#</span><span class="bash"> 文件名</span></span><br><span class="line">appendfilename &quot;appendonly.aof&quot;</span><br></pre></td></tr></table></figure><p>通过修改redis.conf重启redis之后：systemctl restart redis_6379。</p><p>再次运行redis的相关操作命令，会发现在指定的<code>dir</code>目录下生成appendonly.aof文件，通过vim查看该文件内容如下</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">*2</span><br><span class="line"><span class="meta">$</span><span class="bash">6</span></span><br><span class="line">SELECT</span><br><span class="line"><span class="meta">$</span><span class="bash">1</span></span><br><span class="line">0</span><br><span class="line">*3</span><br><span class="line"><span class="meta">$</span><span class="bash">3</span></span><br><span class="line">set</span><br><span class="line"><span class="meta">$</span><span class="bash">4</span></span><br><span class="line">name</span><br><span class="line"><span class="meta">$</span><span class="bash">3</span></span><br><span class="line">mic</span><br><span class="line">*3</span><br><span class="line"><span class="meta">$</span><span class="bash">3</span></span><br><span class="line">set</span><br><span class="line"><span class="meta">$</span><span class="bash">4</span></span><br><span class="line">name</span><br><span class="line"><span class="meta">$</span><span class="bash">3</span></span><br><span class="line">123</span><br></pre></td></tr></table></figure><h3 id="AOF配置相关问题解答"><a href="#AOF配置相关问题解答" class="headerlink" title="AOF配置相关问题解答"></a>AOF配置相关问题解答</h3><p><strong>问题1：数据都是实时持久化到磁盘吗？</strong></p><p>虽然每次执行更改Redis数据库内容的操作时，AOF都会将命令记录在AOF文件中，但是事实上，由于操作系统的缓存机制，数据并没有真正地写入硬盘，而是进入了系统的硬盘缓存。在默认情况下系统每30秒会执行一次同步操作。以便将硬盘缓存中的内容真正地写入硬盘。</p><p>在这30秒的过程中如果系统异常退出则会导致硬盘缓存中的数据丢失。一般来说能够启用AOF的前提是业务场景不能容忍这样的数据损失，这个时候就需要Redis在写入AOF文件后主动要求系统将缓存内容同步到硬盘中。在redis.conf中通过如下配置来设置同步机制。</p><table><thead><tr><th><strong>参数</strong></th><th><strong>说明</strong></th></tr></thead><tbody><tr><td>appendfsync everysec</td><td>AOF持久化策略（硬盘缓存到磁盘），默认<strong>everysec</strong> <br /> 1 no  表示不执行fsync，由操作系统保证数据同步到磁盘，速度最快，但是不太安全；  <br /> 2 always  表示每次写入都执行fsync，以保证数据同步到磁盘，效率很低；<br /> 3 everysec表示每秒执行一次fsync，可能会导致丢失这1s数据。通常选择 everysec ，兼顾安全性和效率。</td></tr></tbody></table><p><strong>问题2：文件越来越大，怎么办？</strong></p><p>由于AOF持久化是Redis不断将写命令记录到 AOF 文件中，随着Redis不断的运行，AOF 的文件会越来越大，文件越大，占用服务器内存越大以及 AOF 恢复要求时间越长。</p><p><strong>例如set gupao 666，执行1000次，结果都是gupao=666。</strong></p><p>为了解决这个问题，Redis新增了重写机制，当AOF文件的大小超过所设定的阈值时，Redis就会启动AOF文件的内容压缩，只保留可以恢复数据的最小指令集。</p><p>可以使用命令下面这个命令主动触发重写</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">redis&gt;</span><span class="bash"> bgrewriteaof</span></span><br></pre></td></tr></table></figure><p>AOF 文件重写并不是对原文件进行重新整理，而是直接读取服务器现有的键值对，然后用一条命令去代替之前记录这个键值对的多条命令，生成一个新的文件后去替换原来的 AOF 文件。</p><p><strong>重写触发机制如下</strong></p><table><thead><tr><th><strong>参数</strong></th><th><strong>说明</strong></th></tr></thead><tbody><tr><td>auto-aof-rewrite-percentage</td><td>默认值为100。表示的是当目前的AOF文件大小超过上一次重写时的AOF文件大小的百分之多少时会再次进行重写，如果之前没有重写过，则以启动时AOF文件大小为依据</td></tr><tr><td>auto-aof-rewrite-min-size</td><td>默认64M。表示限制了允许重写的最小AOF文件大小，通常在AOF文件很小的情况下即使其中有很多冗余的命令我们也并不太关心</td></tr></tbody></table><p>在启动时，Redis会逐个执行AOF文件中的命令来将硬盘中的数据载入到内存中，载入的速度相对于RDB会慢一些</p><p><strong>问题：重写过程中，AOF文件被更改了怎么办？</strong></p><p>Redis 可以在 AOF 文件体积变得过大时，自动地在后台对 AOF 进行重写： 重写后的新 AOF 文件包含了恢复当前数据集所需的最小命令集合。 </p><p>重写的流程是这样，</p><ul><li>主进程会fork一个子进程出来进行AOF重写，这个重写过程并不是基于原有的aof文件来做的，而是有点类似于快照的方式，全量遍历内存中的数据，然后逐个序列到aof文件中。</li><li>在fork子进程这个过程中，服务端仍然可以对外提供服务，<strong>那这个时候重写的aof文件的数据和redis内存数据不一致了怎么办？</strong>不用担心，这个过程中，主进程的数据更新操作，会缓存到<strong>aof_rewrite_buf</strong>中，也就是单独开辟一块缓存来存储重写期间收到的命令，当子进程重写完以后再把缓存中的数据追加到新的aof文件。</li><li>当所有的数据全部追加到新的aof文件中后，把新的aof文件重命名正式的文件名字，此后所有的操作都会被写入新的aof文件。</li><li>如果在rewrite过程中出现故障，不会影响原来aof文件的正常工作，只有当rewrite完成后才会切换文件。因此这个rewrite过程是比较可靠的。</li></ul><p><img src="https://mic-blob-bucket.oss-cn-beijing.aliyuncs.com/202110151355374.png" alt="img"></p><center>图4-26</center><p>Redis允许同时开启AOF和RDB，既保证了数据安全又使得进行备份等操作十分容易。如果同时开启后，Redis重启会使用AOF文件来恢复数据，因为AOF方式的持久化可能丢失的数据更少。</p><h3 id="AOF的优劣势"><a href="#AOF的优劣势" class="headerlink" title="AOF的优劣势"></a>AOF的优劣势</h3><p><strong>优点：</strong></p><p>1、AOF 持久化的方法提供了多种的同步频率，即使使用默认的同步频率每秒同步一次，Redis 最多也就丢失 1 秒的数据而已。</p><p><strong>缺点：</strong></p><p>1、对于具有相同数据的的Redis，AOF 文件通常会比 RDB 文件体积更大（RDB存的是数据快照）。</p><p>2、虽然 AOF 提供了多种同步的频率，默认情况下，每秒同步一次的频率也具有较高的性能。在高并发的情况下，RDB 比 AOF 具好更好的性能保证。</p>]]></content>
      
      
      <categories>
          
          <category> Redis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 高并发 </tag>
            
            <tag> 面试题 </tag>
            
            <tag> Redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>阿里P8面试官：如何设计一个扛住千万级并发的架构（超级详细）-续</title>
      <link href="/posts/2734400627/"/>
      <url>/posts/2734400627/</url>
      
        <content type="html"><![CDATA[<p>在上一篇文章中，详细分析了设计一个千万级并发架构所需要思考的问题，以及解决方案。<br>在这一片文章中，我们主要分析如何在职场足够用户数量的情况下，同步提升架构的性能降低平均响应时间。</p><h1 id="如何降低RT的值"><a href="#如何降低RT的值" class="headerlink" title="如何降低RT的值"></a>如何降低RT的值</h1><p>继续看上面这个图，一个请求只有等到tomcat容器中的应用执行完成才能返回，而请求在执行过程中会做什么事情呢？</p><ul><li>查询数据库</li><li>访问磁盘数据</li><li>进行内存运算</li><li>调用远程服务</li></ul><p>这些操作每一个步骤都会消耗时间，当前客户端的请求只有等到这些操作都完成之后才能返回，所以降低RT的方法，就是优化业务逻辑的处理。</p><h2 id="数据库瓶颈的优化"><a href="#数据库瓶颈的优化" class="headerlink" title="数据库瓶颈的优化"></a>数据库瓶颈的优化</h2><p>当18000个请求进入到服务端并且被接收后，开始执行业务逻辑处理，那么必然会查询数据库。</p><p>每个请求至少都有一次查询数据库的操作，多的需要查询3~5次以上，我们假设按照3次来计算，那么每秒会对数据库形成54000个请求，假设一台数据库服务器每秒支撑10000个请求（影响数据库的请求数量有很多因素，比如数据库表的数据量、数据库服务器本身的系统性能、查询语句的复杂度），那么需要6台数据库服务器才能支撑每秒10000个请求。</p><p>除此之外，数据库层面还有涉及到其他的优化方案。</p><ul><li><p>首先是Mysql的最大连接数设置，大家可能遇到过<code>MySQL: ERROR 1040: Too many connections</code>这样的问题，原因就是访问量过高，连接数耗尽了。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">show</span> variables <span class="keyword">like</span> <span class="string">&#x27;%max_connections%&#x27;</span>;</span><br></pre></td></tr></table></figure><p>如果服务器的并发连接请求量比较大，建议调高此值，以增加并行连接数量，当然这建立在机器能支撑的情况下，因为如果连接数越多，介于MySQL会为每个连接提供连接缓冲区，就会开销越多的内存，所以要适当调整该值，不能盲目提高设值。</p></li><li><p>数据表数据量过大，比如达到几千万甚至上亿，这种情况下sql的优化已经毫无意义了，因为这么大的数据量查询必然会涉及到运算。</p><ul><li><p>可以缓存来解决读请求并发过高的问题，一般来说对于数据库的读写请求也都遵循2/8法则，在每秒54000个请求中，大概有43200左右是读请求，这些读请求中基本上90%都是可以通过缓存来解决。</p></li><li><p>分库分表，减少单表数据量，单表数据量少了，那么查询性能就自然得到了有效的提升</p></li><li><p>读写分离，避免事务操作对查询操作带来的性能影响</p><blockquote><ul><li><p>写操作本身耗费资源</p><p>数据库写操作为IO写入，写入过程中通常会涉及唯一性校验、建索引、索引排序等操作，对资源消耗比较大。一次写操作的响应时间往往是读操作的几倍甚至几十倍。</p></li><li><p>锁争用</p><p>写操作很多时候需要加锁，包括表级锁、行级锁等，这类锁都是排他锁，一个会话占据排它锁之后，其他会话是不能读取数据的，这会会极大影响数据读取性能。</p><p>所以MYSQL部署往往会采用读写分离方式，主库用来写入数据及部分时效性要求很高的读操作，从库用来承接大部分读操作，这样数据库整体性能能够得到大幅提升。</p></li></ul></blockquote></li></ul></li><li><p>不同类型的数据采用不同的存储库，</p><ul><li>MongoDB  nosql 文档化存储</li><li>Redis  nosql  key-value存储</li><li>HBase nosql， 列式存储，其实本质上有点类似于key-value数据库。</li><li>cassandra，Cassandra 是一个来自 Apache 的分布式数据库，具有高度可扩展性，可用于管理大量的结构化数据</li><li>TIDB，是PingCAP公司自主设计、研发的开源分布式关系型数据库，是一款同时支持在线事务处理与在线分析处理 (Hybrid Transactional and Analytical Processing, HTAP) 的融合型分布式数据库产品</li></ul></li></ul><blockquote><p>为什么把mysql数据库中的数据放redis缓存中能提升性能？</p><ol><li>Redis存储的是k-v格式的数据。时间复杂度是O(1),常数阶,而mysql引擎的底层实现是B+TREE，时间复杂度是O(logn）是对数阶的。Redis会比Mysql快一点点。</li><li>Mysql数据存储是存储在表中，查找数据时要先对表进行全局扫描或根据索引查找，这涉及到磁盘的查找，磁盘查找如果是单点查找可能会快点，但是顺序查找就比较慢。而redis不用这么麻烦，本身就是存储在内存中，会根据数据在内存的位置直接取出。</li><li>Redis是单线程的多路复用IO,单线程避免了线程切换的开销，而多路复用IO避免了IO等待的开销，在多核处理器下提高处理器的使用效率可以对数据进行分区，然后每个处理器处理不同的数据。</li></ol></blockquote><ul><li><p>池化技术，减少频繁创建数据库连接的性能损耗。</p><p>每次进行数据库操作之前，先建立连接然后再进行数据库操作，最后释放连接。这个过程涉及到网络通信的延时，频繁创建连接对象和销毁对象的性能开销等，当请求量较大时，这块带来的性能影响非常大。</p></li></ul><p><img src="https://mic-blob-bucket.oss-cn-beijing.aliyuncs.com/202110171350704.png" alt="数据存储"></p><h2 id="磁盘数据访问优化"><a href="#磁盘数据访问优化" class="headerlink" title="磁盘数据访问优化"></a>磁盘数据访问优化</h2><p>对于磁盘的操作，无非就是读和写。</p><p>比如对于做交易系统的场景来说，一般会设计到对账文件的解析和写入。而对于磁盘的操作，优化方式无非就是</p><ul><li><p>磁盘的页缓存，可以借助缓存 I/O ，充分利用系统缓存，降低实际 I/O 的次数。</p></li><li><p>顺序读写，可以用追加写代替随机写，减少寻址开销，加快 I/O 写的速度。</p></li><li><p>SSD代替HDD，固态硬盘的I/O效率远远高于机械硬盘。</p></li><li><p>在需要频繁读写同一块磁盘空间时，可以用 mmap （内存映射，）代替 read/write，减少内存的拷贝次数</p></li><li><p>在需要同步写的场景中，尽量将写请求合并，而不是让每个请求都同步写入磁盘，即可以用 fsync() 取代 O_SYNC</p></li></ul><h2 id="合理利用内存"><a href="#合理利用内存" class="headerlink" title="合理利用内存"></a>合理利用内存</h2><p>充分利用内存缓存，把一些经常访问的数据和对象保存在内存中，这样可以避免重复加载或者避免数据库访问带来的性能损耗。</p><h2 id="调用远程服务"><a href="#调用远程服务" class="headerlink" title="调用远程服务"></a>调用远程服务</h2><p>远程服务调用，影响到IO性能的因素有。</p><ul><li>远程调用等待返回结果的阻塞<ul><li>异步通信</li></ul></li><li>网络通信的耗时<ul><li>内网通信</li><li>增加网络带宽</li></ul></li><li>远程服务通信的稳定性</li></ul><h2 id="异步化架构"><a href="#异步化架构" class="headerlink" title="异步化架构"></a>异步化架构</h2><p>微服务中的逻辑复杂处理时间长的情况，在高并发量下，导致服务线程消耗尽，不能再创建线程处理请求。对这种情况的优化，除了在程序上不断调优(数据库调优，算法调优，缓存等等)，可以考虑在架构上做些调整，先返回结果给客户端，让用户可以继续使用客户端的其他操作，再把服务端的复杂逻辑处理模块做异步化处理。这种异步化处理的方式适合于客户端对处理结果不敏感不要求实时的情况，比如群发邮件、群发消息等。</p><p>异步化设计的解决方案： 多线程、MQ。</p><h1 id="应用服务的拆分"><a href="#应用服务的拆分" class="headerlink" title="应用服务的拆分"></a>应用服务的拆分</h1><p>除了上述的手段之外，业务系统往微服务化拆分也非常有必要，原因是：</p><ul><li>随着业务的发展，应用程序本身的复杂度会不断增加，同样会产生熵增现象。</li><li>业务系统的功能越来越多，参与开发迭代的人员也越多，多个人维护一个非常庞大的项目，很容易出现问题。</li><li>单个应用系统很难实现横向扩容，并且由于服务器资源有限，导致所有的请求都集中请求到某个服务器节点，造成资源消耗过大，使得系统不稳定</li><li>测试、部署成本越来越高</li><li>…..</li></ul><p>其实，最终要的是，单个应用在性能上的瓶颈很难突破，也就是说如果我们要支持18000QPS，单个服务节点肯定无法支撑，所以服务拆分的好处，就是可以利用多个计算机阶段组成一个大规模的分布式计算网络，通过网络通信的方式完成一整套业务逻辑。</p><p><img src="https://mic-blob-bucket.oss-cn-beijing.aliyuncs.com/202110171350400.png" alt="img"></p><h2 id="如何拆分服务"><a href="#如何拆分服务" class="headerlink" title="如何拆分服务"></a>如何拆分服务</h2><p>如何拆分服务，这个问题看起来简单，很多同学会说，直接按照业务拆分啊。</p><p>但是实际在实施的时候，会发现拆分存在一些边界性问题，比如有些数据模型可以存在A模块，也可以存在B模块，这个时候怎么划分呢？另外，服务拆分的粒度应该怎么划分？</p><p>一般来说，服务的拆分是按照业务来实现的，然后基于DDD来指导微服务的边界划分。<strong>领域驱动就是一套方法论，通过领域驱动设计方法论来定义领域模型，从而确定业务边界和应用边界，保证业务模型和代码模型的一致性。</strong>不管是DDD还是微服务，都要遵循软件设计的基本原则：<strong>高内聚低耦合</strong>。服务内部高内聚，服务之间低耦合，实际上一个领域服务对应了一个功能集合，这些功能一定是有一些共性的。比如，订单服务，那么创建订单、修改订单、查询订单列表，领域的边界越清晰，功能也就越内聚，服务之间的耦合性也就越低。</p><p>服务拆分还需要根据当前技术团队和公司所处的状态来进行。</p><p>如果是初创团队，不需要过分的追求微服务，否则会导致业务逻辑过于分散，技术架构太过负载，再加上团队的基础设施还不够完善，导致整个交付的时间拉长，对公司的发展来说会造成较大的影响。所以在做服务拆分的时候还需要考虑几个因素。</p><ul><li>当前公司业务所处领域的市场性质，如果是市场较为敏感的项目，前期应该是先出来东西，然后再去迭代和优化。</li><li>开发团队的成熟度，团队技术能否能够承接。</li><li>基础能力是否足够，比如Devops、运维、测试自动化等基础能力。 团队是否有能力来支撑大量服务实例运行带来的运维复杂度，是否可以做好服务的监控。</li><li>测试团队的执行效率，如果测试团队不能支持自动化测试、自动回归、压力测试等手段来提高测试效率，那必然会带来测试工作量的大幅度提升从而导致项目上线周期延期</li></ul><p>如果是针对一个老的系统进行改造，那可能涉及到的风险和问题更多，所以要开始着手改动之前，需要考虑几个步骤：拆分前准备阶段，设计拆分改造方案，实施拆分计划</p><ul><li><p>拆分之前，先梳理好当前的整个架构，以及各个模块的依赖关系，还有接口</p><p>准备阶段主要是梳理清楚了依赖关系和接口，就可以思考如何来拆，第一刀切在哪儿里，即能达到快速把一个复杂单体系统变成两个更小系统的目标，又能对系统的现有业务影响最小。要尽量避免构建出一个分布式的单体应用，一个包含了一大堆互相之间紧耦合的服务，却又必须部署在一起的所谓分布式系统。没分析清楚就强行拆，可能就一不小心剪断了大动脉，立马搞出来一个 A 类大故障，后患无穷。</p></li><li><p>不同阶段拆分要点不同，每个阶段的关注点要聚焦</p><p>拆分本身可以分成三个阶段，核心业务和非业务部分的拆分、核心业务的调整设计、核心业务内部的拆分。</p><ul><li><p>第一阶段将核心业务瘦身，把非核心的部分切开，减少需要处理的系统大小；</p></li><li><p>第二阶段。重新按照微服务设计核心业务部分；</p></li><li><p>第三阶段把核心业务部分重构设计落地。</p></li></ul><p>拆分的方式也有三个：代码拆分、部署拆分、数据拆分。</p></li></ul><p>另外，每个阶段需要聚焦到一两个具体的目标，否则目标太多反而很难把一件事儿做通透。例如某个系统的微服务拆分，制定了如下的几个目标：</p><ol><li>性能指标（吞吐和延迟）：核心交易吞吐提升一倍以上（TPS：1000-&gt;10000），A 业务延迟降低一半（Latency：250ms-&gt;125ms），B 业务延迟降低一半（Latency：70ms-&gt;35ms）。</li><li>稳定性指标（可用性，故障恢复时间）：可用性&gt;=99.99%，A 类故障恢复时间&lt;=15 分钟，季度次数&lt;=1 次。</li><li>质量指标：编写完善的产品需求文档、设计文档、部署运维文档，核心交易部分代码 90%以上单测覆盖率和 100%的自动化测试用例和场景覆盖，实现可持续的性能测试基准环境和长期持续性能优化机制。</li><li>扩展性指标：完成代码、部署、运行时和数据多个维度的合理拆分，对于核心系统重构后的各块业务和交易模块、以及对应的各个数据存储，都可以随时通过增加机器资源实现伸缩扩展。</li><li>可维护性指标：建立全面完善的监控指标、特别是全链路的实时性能指标数据，覆盖所有关键业务和状态，缩短监控报警响应处置时间，配合运维团队实现容量规划和管理，出现问题时可以在一分钟内拉起系统或者回滚到上一个可用版本（启动时间&lt;=1 分钟）。</li><li>易用性指标，通过重构实现新的 API 接口既合理又简单，极大的满足各个层面用户的使用和需要，客户满意度持续上升。</li><li>业务支持指标：对于新的业务需求功能开发，在保障质量的前提下，开发效率提升一倍，开发资源和周期降低一半。</li></ol><p>当然，不要期望一次性完成所有目标，每一个阶段可以选择一个两个优先级高的目标进行执行。</p><p><img src="https://mic-blob-bucket.oss-cn-beijing.aliyuncs.com/202110171350615.png" alt="img"></p><h2 id="微服务化架构带来的问题"><a href="#微服务化架构带来的问题" class="headerlink" title="微服务化架构带来的问题"></a>微服务化架构带来的问题</h2><p>微服务架构首先是一个分布式的架构，其次我们要暴露和提供业务服务能力，然后我们需要考虑围绕这些业务能力的各种非功能性的能力。这些分散在各处的服务本身需要被管理起来，并且对服务的调用方透明，这样就有了服务的注册发现的功能需求。</p><p>同样地，每个服务可能部署了多台机器多个实例，所以，我们需要有路由和寻址的能力，做负载均衡，提升系统的扩展能力。有了这么多对外提供的不同服务接口，我们一样需要有一种机制对他们进行统一的接入控制，并把一些非业务的策略做到这个接入层，比如权限相关的，这就是服务网关。同时我们发现随着业务的发展和一些特定的运营活动，比如秒杀大促，流量会出现十倍以上的激增，这时候我们就需要考虑系统容量，服务间的强弱依赖关系，做服务降级、熔断，系统过载保护等措施。</p><p>以上这些由于微服务带来的复杂性，导致了应用配置、业务配置，都被散落到各处，所以分布式配置中心的需求也出现了。最后，系统分散部署以后，所有的调用都跨了进程，我们还需要有能在线上做链路跟踪，性能监控的一套技术，来协助我们时刻了解系统内部的状态和指标，让我们能够随时对系统进行分析和干预。</p><p><img src="https://mic-blob-bucket.oss-cn-beijing.aliyuncs.com/202110171351673.png" alt="image-20210624133950124"></p><h2 id="整体架构图"><a href="#整体架构图" class="headerlink" title="整体架构图"></a>整体架构图</h2><p>基于上述从微观到宏观的整体分析，我们基本上能够设计出一个整体的架构图。</p><ul><li><p>接入层，外部请求到内部系统之间的关口，所有请求都必须经过api 网关。</p></li><li><p>应用层，也叫聚合层，为相关业务提供聚合接口，它会调用中台服务进行组装。</p></li><li><p>中台服务，也是业务服务层，以业务为纬度提供业务相关的接口。中台的本质是为整个架构提供复用的能力，比如评论系统，在咕泡云课堂和Gper社区都需要，那么这个时候评论系统为了设计得更加可复用性，就不能耦合云课堂或者Gper社区定制化的需求，那么作为设计评论中台的人，就不需要做非常深度的思考，如何提供一种针对不同场景都能复用的能力。</p><p>你会发现，当这个服务做到机制的时候，就变成了一个baas服务。</p><blockquote><p><strong>服务商</strong>为<strong>客户</strong>(开发者)提供整合云后端的服务，如提供文件存储、数据存储、推送服务、身份验证服务等功能，以帮助开发者快速开发应用。</p></blockquote></li></ul><p><img src="https://mic-blob-bucket.oss-cn-beijing.aliyuncs.com/202110091513908.png" alt="image-20210624152616146"></p><h1 id="了解什么是高并发"><a href="#了解什么是高并发" class="headerlink" title="了解什么是高并发"></a>了解什么是高并发</h1><p>总结一下什么是高并发。</p><p>高并发并没有一个具体的定义，高并发主要是形容突发流量较高的场景。</p><p>如果面试的过程中，或者在实际工作中，你们领导或者面试官问你一个如何设计承接千万级流量的系统时，你应该要按照我说的方法去进行逐一分析。</p><ul><li>一定要形成可以量化的数据指标，比如QPS、DAU、总用户数、TPS、访问峰值</li><li>针对这些数据情况，开始去设计整个架构方案</li><li>接着落地执行</li></ul><h2 id="高并发中的宏观指标"><a href="#高并发中的宏观指标" class="headerlink" title="高并发中的宏观指标"></a>高并发中的宏观指标</h2><p>一个满足高并发系统，不是一味追求高性能，至少需要满足三个宏观层面的目标：</p><ul><li>高性能，性能体现了系统的并行处理能力，在有限的硬件投入下，提高性能意味着节省成本。同时，性能也反映了用户体验，响应时间分别是 100 毫秒和 1 秒，给用户的感受是完全不同的。</li><li>高可用，表示系统可以正常服务的时间。一个全年不停机、无故障；另一个隔三差五出现上事故、宕机，用户肯定选择前者。另外，如果系统只能做到 90%可用，也会大大拖累业务。</li><li>高扩展，表示系统的扩展能力，流量高峰时能否在短时间内完成扩容，更平稳地承接峰值流量，比如双 11 活动、明星离婚等热点事件。</li></ul><p><img src="https://mic-blob-bucket.oss-cn-beijing.aliyuncs.com/202110091513525.png" alt="image-20210624211728937"></p><h2 id="微观指标"><a href="#微观指标" class="headerlink" title="微观指标"></a>微观指标</h2><p><strong>性能指标</strong></p><p>通过性能指标可以度量目前存在的性能问题，同时作为性能优化的评估依据。一般来说，会采用一段时间内的接口响应时间作为指标。</p><p>1、平均响应时间：最常用，但是缺陷很明显，对于慢请求不敏感。比如 1 万次请求，其中 9900 次是 1ms，100 次是 100ms，则平均响应时间为 1.99ms，虽然平均耗时仅增加了 0.99ms，但是 1%请求的响应时间已经增加了 100 倍。</p><p>2、TP90、TP99 等分位值：将响应时间按照从小到大排序，TP90 表示排在第 90 分位的响应时间， 分位值越大，对慢请求越敏感。</p><p><img src="https://mic-blob-bucket.oss-cn-beijing.aliyuncs.com/202110091513882.jpeg" alt="img"></p><p><strong>可用性指标</strong></p><p>高可用性是指系统具有较高的无故障运行能力，可用性 = 平均故障时间 / 系统总运行时间，一般使用几个 9 来描述系统的可用性。</p><p>对于高并发系统来说，最基本的要求是：保证 3 个 9 或者 4 个 9。原因很简单，如果你只能做到 2 个 9，意味着有 1%的故障时间，像一些大公司每年动辄千亿以上的 GMV 或者收入，1%就是 10 亿级别的业务影响。</p><p><strong>可扩展性指标</strong></p><p>面对突发流量，不可能临时改造架构，最快的方式就是增加机器来线性提高系统的处理能力。</p><p>对于业务集群或者基础组件来说，扩展性 = 性能提升比例 / 机器增加比例，理想的扩展能力是：资源增加几倍，性能提升几倍。通常来说，扩展能力要维持在 70%以上。</p><p>但是从高并发系统的整体架构角度来看，扩展的目标不仅仅是把服务设计成无状态就行了，因为当流量增加 10 倍，业务服务可以快速扩容 10 倍，但是数据库可能就成为了新的瓶颈。</p><p>像 MySQL 这种有状态的存储服务通常是扩展的技术难点，如果架构上没提前做好规划（垂直和水平拆分），就会涉及到大量数据的迁移。</p><p>因此，高扩展性需要考虑：服务集群、数据库、缓存和消息队列等中间件、负载均衡、带宽、依赖的第三方等，当并发达到某一个量级后，上述每个因素都可能成为扩展的瓶颈点。</p><h2 id="实践方案"><a href="#实践方案" class="headerlink" title="实践方案"></a>实践方案</h2><p>通用设计方法</p><p><strong>纵向扩展（scale-up）</strong></p><p>它的目标是提升单机的处理能力，方案又包括：</p><p>1、提升单机的硬件性能：通过增加内存、CPU 核数、存储容量、或者将磁盘升级成 SSD 等堆硬件的方式来提升。</p><p>2、提升单机的软件性能：使用缓存减少 IO 次数，使用并发或者异步的方式增加吞吐量。</p><p><strong>横向扩展（scale-out）</strong></p><p>因为单机性能总会存在极限，所以最终还需要引入横向扩展，通过集群部署以进一步提高并发处理能力，又包括以下 2 个方向：</p><p>1、做好分层架构：这是横向扩展的提前，因为高并发系统往往业务复杂，通过分层处理可以简化复杂问题，更容易做到横向扩展。</p><p>2、各层进行水平扩展：无状态水平扩容，有状态做分片路由。业务集群通常能设计成无状态的，而数据库和缓存往往是有状态的，因此需要设计分区键做好存储分片，当然也可以通过主从同步、读写分离的方案提升读性能。</p><h3 id="高性能实践方案"><a href="#高性能实践方案" class="headerlink" title="高性能实践方案"></a>高性能实践方案</h3><p>1、集群部署，通过负载均衡减轻单机压力。</p><p>2、多级缓存，包括静态数据使用 CDN、本地缓存、分布式缓存等，以及对缓存场景中的热点 key、缓存穿透、缓存并发、数据一致性等问题的处理。</p><p>3、分库分表和索引优化，以及借助搜索引擎解决复杂查询问题。</p><p>4、考虑 NoSQL 数据库的使用，比如 HBase、TiDB 等，但是团队必须熟悉这些组件，且有较强的运维能力。</p><p>5、异步化，将次要流程通过多线程、MQ、甚至延时任务进行异步处理。</p><p>6、限流，需要先考虑业务是否允许限流（比如秒杀场景是允许的），包括前端限流、Nginx 接入层的限流、服务端的限流。</p><p>7、对流量进行削峰填谷，通过 MQ 承接流量。</p><p>8、并发处理，通过多线程将串行逻辑并行化。</p><p>9、预计算，比如抢红包场景，可以提前计算好红包金额缓存起来，发红包时直接使用即可。</p><p>10、缓存预热，通过异步任务提前预热数据到本地缓存或者分布式缓存中。</p><p>11、减少 IO 次数，比如数据库和缓存的批量读写、RPC 的批量接口支持、或者通过冗余数据的方式干掉 RPC 调用。</p><p>12、减少 IO 时的数据包大小，包括采用轻量级的通信协议、合适的数据结构、去掉接口中的多余字段、减少缓存 key 的大小、压缩缓存 value 等。</p><p>13、程序逻辑优化，比如将大概率阻断执行流程的判断逻辑前置、For 循环的计算逻辑优化，或者采用更高效的算法。</p><p>14、各种池化技术的使用和池大小的设置，包括 HTTP 请求池、线程池（考虑 CPU 密集型还是 IO 密集型设置核心参数）、数据库和 Redis 连接池等。</p><p>15、JVM 优化，包括新生代和老年代的大小、GC 算法的选择等，尽可能减少 GC 频率和耗时。</p><p>16、锁选择，读多写少的场景用乐观锁，或者考虑通过分段锁的方式减少锁冲突。</p><h3 id="高可用实践方案"><a href="#高可用实践方案" class="headerlink" title="高可用实践方案"></a>高可用实践方案</h3><p>1、对等节点的故障转移，Nginx 和服务治理框架均支持一个节点失败后访问另一个节点。</p><p>2、非对等节点的故障转移，通过心跳检测并实施主备切换（比如 redis 的哨兵模式或者集群模式、MySQL 的主从切换等）。</p><p>3、接口层面的超时设置、重试策略和幂等设计。</p><p>4、降级处理：保证核心服务，牺牲非核心服务，必要时进行熔断；或者核心链路出问题时，有备选链路。</p><p>5、限流处理：对超过系统处理能力的请求直接拒绝或者返回错误码。</p><p>6、MQ 场景的消息可靠性保证，包括 producer 端的重试机制、broker 侧的持久化、consumer 端的 ack 机制等。</p><p>7、灰度发布，能支持按机器维度进行小流量部署，观察系统日志和业务指标，等运行平稳后再推全量。</p><p>8、监控报警：全方位的监控体系，包括最基础的 CPU、内存、磁盘、网络的监控，以及 Web 服务器、JVM、数据库、各类中间件的监控和业务指标的监控。</p><p>9、灾备演练：类似当前的“混沌工程”，对系统进行一些破坏性手段，观察局部故障是否会引起可用性问题。</p><p>高可用的方案主要从冗余、取舍、系统运维 3 个方向考虑，同时需要有配套的值班机制和故障处理流程，当出现线上问题时，可及时跟进处理。</p><h3 id="高扩展的实践方案"><a href="#高扩展的实践方案" class="headerlink" title="高扩展的实践方案"></a>高扩展的实践方案</h3><p>1、合理的分层架构：比如上面谈到的互联网最常见的分层架构，另外还能进一步按照数据访问层、业务逻辑层对微服务做更细粒度的分层（但是需要评估性能，会存在网络多一跳的情况）。</p><p>2、存储层的拆分：按照业务维度做垂直拆分、按照数据特征维度进一步做水平拆分（分库分表）。</p><p>3、业务层的拆分：最常见的是按照业务维度拆（比如电商场景的商品服务、订单服务等），也可以按照核心接口和非核心接口拆，还可以按照请求去拆（比如 To C 和 To B，APP 和 H5）。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 高并发 </tag>
            
            <tag> 架构设计 </tag>
            
            <tag> 面试题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>阿里P8面试官：如何设计一个扛住千万级并发的架构？</title>
      <link href="/posts/1568521894/"/>
      <url>/posts/1568521894/</url>
      
        <content type="html"><![CDATA[<p>大家先思考一个问题，这也是在面试过程中经常遇到的问题。</p><blockquote><p>如果你们公司现在的产品能够支持10W用户访问，你们老板突然和你说，融到钱了，会大量投放广告，预计在1个月后用户量会达到1000W，如果这个任务交给你，你应该怎么做？</p></blockquote><h1 id="1000W用户的问题分解"><a href="#1000W用户的问题分解" class="headerlink" title="1000W用户的问题分解"></a>1000W用户的问题分解</h1><p>如何支撑1000W用户其实是一个非常抽象的问题，对于技术开发来说，我们需要一个非常明确的对于执行关键业务上的性能指标数据，比如，高峰时段下对于事务的响应时间、并发用户数、QPS、成功率、以及基本指标要求等，这些都 必须要非常明确，只有这样才能够指导整个架构的改造和优化。所以，如果大家接到这样一个问题，首先需要去定位到问题的本质，也就是首先得知道一些可量化的数据指标。</p><ul><li><p>如果有过往的相似业务交易历史数据经验，你需要尽量参考，处理这些收集到的原始数据（日志），从而分析出高峰时段，以及该时段下的交易行为，交易规模等，得到你想要看清楚的需求细节</p></li><li><p>另外一种情况，就是没有相关的数据指标作为参考，这个时候就需要经验来分析。比如可以参考一些类似行业的比较成熟的业务交易模型（比如银行业的日常交易活动或交通行业售检票交易活动）或者干脆遵循“2/8”原则和“2/5/8”原则来直接下手实践。</p><blockquote><ul><li>当用户能够在2秒以内得到响应时，会感觉系统的响应很快；</li><li>当用户在2-5秒之间得到响应时，会感觉系统的响应速度还可以；</li><li>当用户在5-8秒以内得到响应时，会感觉系统的响应速度很慢，但是还可以接受；</li><li>而当用户在超过8秒后仍然无法得到响应时，会感觉系统糟透了，或者认为系统已经失去响应，而选择离开这个Web站点，或者发起第二次请求。</li></ul></blockquote></li></ul><p>在估算响应时间、并发用户数、TPS、成功率这些关键指标的同时，你仍需要关心具体的业务功能维度上的需求，每个业务功能都有各自的特点，比如有些场景可以不需要同步返回明确执行结果，有些业务场景可以接受返回“系统忙，请等待！”这样暴力的消息，以避免过大的处理流量所导致的大规模瘫痪，因此，学会平衡这些指标之间的关系是必要的，大多数情况下最好为这些指标做一个优先级排序，并且尽量只考察几个优先级高的指标要求。(SLA服务等级)</p><blockquote><p><strong>SLA</strong>：Service-Level Agreement的缩写，意思是服务等级协议。服务的SLA是服务提供者对服务消费者的正式承诺，是衡量服务能力等级的关键项。服务SLA中定义的项必须是可测量的，有明确的测量方法。</p></blockquote><p><img src="https://mic-blob-bucket.oss-cn-beijing.aliyuncs.com/202110171350947.png" alt="image-20210623165109183"></p><h2 id="并发中相关概念的解释"><a href="#并发中相关概念的解释" class="headerlink" title="并发中相关概念的解释"></a>并发中相关概念的解释</h2><p>在分析上述问题之前，先给大家普及一下，系统相关的一些关键衡量指标。</p><h3 id="TPS"><a href="#TPS" class="headerlink" title="TPS"></a>TPS</h3><p>TPS（Transaction Per Second）每秒处理的事务数。</p><p>站在宏观角度来说，一个事务是指客户端向服务端发起一个请求，并且等到请求返回之后的整个过程。从客户端发起请求开始计时，等到收到服务器端响应结果后结束计时，在计算这个时间段内总共完成的事务个数，我们称为TPS。</p><p>站在微观角度来说，一个数据库的事务操作，从开始事务到事务提交完成，表示一个完整事务，这个是数据库层面的TPS。</p><h3 id="QPS"><a href="#QPS" class="headerlink" title="QPS"></a>QPS</h3><p>QPS（Queries Per Second）每秒查询数，表示服务器端每秒能够响应的查询次数。这里的查询是指用户发出请求到服务器做出响应成功的次数，可以简单认为每秒钟的Request数量。</p><p>针对单个接口而言，TPS和QPS是相等的。如果从宏观层面来说，用户打开一个页面到页面渲染结束代表一个TPS，那这个页面中会调用服务器很多次，比如加载静态资源、查询服务器端的渲染数据等，就会产生两个QPS，因此，一个TPS中可能会包含多个QPS。</p><blockquote><p><strong>QPS=并发数/平均响应时间</strong></p></blockquote><p><img src="https://mic-blob-bucket.oss-cn-beijing.aliyuncs.com/202110171350727.png" alt="image-20210622180649041"></p><h3 id="RT"><a href="#RT" class="headerlink" title="RT"></a>RT</h3><p>RT（Response Time），表示客户端发起请求到服务端返回的时间间隔，一般表示平均响应时间。</p><h3 id="并发数"><a href="#并发数" class="headerlink" title="并发数"></a>并发数</h3><p>并发数是指系统同时能处理的请求数量。</p><p>需要注意，并发数和QPS不要搞混了，QPS表示每秒的请求数量，而并发数是系统同时处理的请求数量，并发数量会大于QPS，因为服务端的一个连接需要有一个处理时长，在这个请求处理结束之前，这个连接一直占用。</p><p>举个例子，如果QPS=1000，表示每秒钟客户端会发起1000个请求到服务端，而如果一个请求的处理耗时是3s，那么意味着总的并发=1000*3=3000，也就是服务端会同时有3000个并发。</p><h3 id="计算方法"><a href="#计算方法" class="headerlink" title="计算方法"></a>计算方法</h3><p>上面说的这些指标，怎么计算呢？举个例子。</p><p>假设在10点到11点这一个小时内，有200W个用户访问我们的系统，假设平均每个用户请求的耗时是3秒，那么计算的结果如下：</p><ul><li>QPS=2000000/60*60 = 556 （表示每秒钟会有556个请求发送到服务端）</li><li>RT=3s（每个请求的平均响应时间是3秒）</li><li>并发数=556*3=1668</li></ul><p>从这个计算过程中发现，随着RT的值越大，那么并发数就越多，而并发数代表着服务器端同时处理的连接请求数量，也就意味服务端占用的连接数越多，这些链接会消耗内存资源以及CPU资源等。所以RT值越大系统资源占用越大，同时也意味着服务端的请求处理耗时较长。</p><p>但实际情况是，RT值越小越好，比如在游戏中，至少做到100ms左右的响应才能达到最好的体验，对于电商系统来说，3s左右的时间是能接受的，那么如何缩短RT的值呢？</p><h2 id="按照2-8法则来推算1000w用户的访问量"><a href="#按照2-8法则来推算1000w用户的访问量" class="headerlink" title="按照2/8法则来推算1000w用户的访问量"></a>按照2/8法则来推算1000w用户的访问量</h2><p>继续回到最开始的问题，假设没有历史数据供我们参考，我们可以使用2/8法则来进行预估。</p><ul><li><p>1000W用户，每天来访问这个网站的用户占到20%，也就是每天有200W用户来访问。</p></li><li><p>假设平均每个用户过来点击50次，那么总共的PV=1亿。</p></li><li><p>一天是24小时，根据2/8法则，每天大部分用户活跃的时间点集中在(24*0.2) 约等于5个小时以内，而大部分用户指的是（1亿点击 * 80%）约等于8000W（PV）， 意味着在5个小时以内，大概会有8000W点击进来，也就是每秒大约有4500(8000W/5小时)个请求。</p></li><li><p>4500只是一个平均数字。在这5个小时中，不可能请求是非常平均的，有可能会存在大量的用户集中访问（比如像淘宝这样的网站，日访问峰值的时间点集中在下午14：00、以及晚上21：00，其中21：00是一天中活跃的峰值），一般情况下访问峰值是平均访问请求的3倍到4倍左右（这个是经验值），我们按照4倍来计算。那么在这5个小时内有可能会出现每秒18000个请求的情况。也就是说，问题由原本的支撑1000W用户，变成了一个具体的问题，<strong>就是服务器端需要能够支撑每秒18000个请求</strong>（QPS=18000）</p></li></ul><p><img src="https://mic-blob-bucket.oss-cn-beijing.aliyuncs.com/202110171350543.png" alt="image-20210622160313561"></p><p><img src="https://mic-blob-bucket.oss-cn-beijing.aliyuncs.com/202110171350921.png" alt="image-20210622160320454"></p><h2 id="服务器压力预估"><a href="#服务器压力预估" class="headerlink" title="服务器压力预估"></a>服务器压力预估</h2><p>大概预估出了后端服务器需要支撑的最高并发的峰值之后，就需要从整个系统架构层面进行压力预估，然后配置合理的服务器数量和架构。既然是这样，那么首先需要知道一台服务器能够扛做多少的并发，那这个问题怎么去分析呢？我们的应用是部署在Tomcat上，所以需要从Tomcat本身的性能下手。</p><p>下面这个图表示Tomcat的工作原理，该图的说明如下。</p><ul><li><p>LimitLatch是连接控制器，它负责控制Tomcat能够同时处理的最大连接数，在NIO/NIO2的模式中，默认是10000，如果是APR/native，默认是8192</p></li><li><p>Acceptor是一个独立的线程，在run方法中，在while循环中调用socket.accept方法中接收客户端的连接请求，一旦有新的请求过来，accept会返回一个Channel对象，接着把这个Channel对象交给Poller去处理。</p><blockquote><p>Poller 的本质是一个 Selector ，它同样也实现了线程，Poller 在内部维护一个 Channel 数组，它在一个死循环里不断检测 Channel 的数据就绪状态，一旦有 Channel 可读，就生成一个 SocketProcessor 任务对象扔给 Executor 去处理</p></blockquote></li><li><p>SocketProcessor 实现了 Runnable 接口，当线程池在执行SocketProcessor这个任务时，会通过Http11Processor去处理当前这个请求，Http11Processor 读取 Channel 的数据来生成 ServletRequest 对象。</p></li><li><p>Executor 就是线程池，负责运行 SocketProcessor 任务类， SocketProcessor 的 run 方法会调用 Http11Processor 来读取和解析请求数据。我们知道， Http11Processor 是应用层协议的封装，它会调用容器获得响应，再把响应通过 Channel 写出。</p></li></ul><p><img src="https://mic-blob-bucket.oss-cn-beijing.aliyuncs.com/202110171350538.png" alt="image-20210622154519229"></p><p>从这个图中可以得出，限制Tomcat请求数量的因素四个方面。</p><h3 id="当前服务器系统资源"><a href="#当前服务器系统资源" class="headerlink" title="当前服务器系统资源"></a>当前服务器系统资源</h3><p>我想可能大家遇到过类似“Socket/File：Can’t open so many files”的异常，这个就是表示Linux系统中的文件句柄限制。</p><p>在Linux中，每一个TCP连接会占用一个文件描述符（fd），一旦文件描述符超过Linux系统当前的限制，就会提示这个错误。</p><p>我们可以通过下面这条命令来查看一个进程可以打开的文件数量</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ulimit -a 或者 ulimit -n</span><br></pre></td></tr></table></figure><p>open files （-n） 1024 是linux操作系统对一个进程打开的文件句柄数量的限制（也包含打开的套接字数量）</p><p>这里只是对用户级别的限制，其实还有个是对系统的总限制，查看系统总线制：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cat /proc/sys/fs/file-max</span><br></pre></td></tr></table></figure><p>file-max是设置系统所有进程一共可以打开的文件数量 。同时一些程序可以通过<code>setrlimit</code>调用，设置每个进程的限制。如果得到大量使用完文件句柄的错误信息，是应该增加这个值。</p><p>当出现上述异常时，我们可以通过下面的方式来进行修改（针对单个进程的打开数量限制）</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vi /etc/security/limits.conf</span><br><span class="line">  root soft nofile 65535</span><br><span class="line">  root hard nofile 65535</span><br><span class="line">  * soft nofile 65535</span><br><span class="line">  * hard nofile 65535</span><br></pre></td></tr></table></figure><ul><li><code>*</code>代表所有用户、<code>root</code>表示root用户。</li><li>noproc 表示最大进程数量</li><li>nofile代表最大文件打开数量。</li><li>soft/hard，前者当达到阈值时，制作警告，后者会报错。</li></ul><p>另外还要注意，要确保针对进程级别的文件打开数量反问是小于或者等于系统的总限制，否则，我们需要修改系统的总限制。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vi /proc/sys/fs/file-max</span><br></pre></td></tr></table></figure><p>TCP连接对于系统资源最大的开销就是内存。</p><p>因为tcp连接归根结底需要双方接收和发送数据，那么就需要一个读缓冲区和写缓冲区，这两个buffer在linux下最小为4096字节，可通过cat /proc/sys/net/ipv4/tcp_rmem和cat /proc/sys/net/ipv4/tcp_wmem来查看。</p><p>所以，一个tcp连接最小占用内存为4096+4096 = 8k，那么对于一个8G内存的机器，在不考虑其他限制下，最多支持的并发量为：8<em>1024</em>1024/8 约等于100万。此数字为纯理论上限数值，在实际中，由于linux kernel对一些资源的限制，加上程序的业务处理，所以，8G内存是很难达到100万连接的，当然，我们也可以通过增加内存的方式增加并发量。</p><h3 id="Tomcat依赖的JVM的配置"><a href="#Tomcat依赖的JVM的配置" class="headerlink" title="Tomcat依赖的JVM的配置"></a>Tomcat依赖的JVM的配置</h3><p>我们知道Tomcat是Java程序，运行在JVM上，因此我们还需要对JVM做优化，才能更好的提升Tomcat的性能，简单带大家了解一下JVM，如下图所示。</p><p><img src="https://mic-blob-bucket.oss-cn-beijing.aliyuncs.com/202110171350493.png" alt="image-20210623204411021"></p><p>在JVM中，内存划分为堆、程序计数器、本地方发栈、方法区（元空间）、虚拟机栈。</p><h4 id="堆空间说明"><a href="#堆空间说明" class="headerlink" title="堆空间说明"></a>堆空间说明</h4><p>其中，堆内存是JVM内存中最大的一块区域，几乎所有的对象和数组都会被分配到堆内存中，它被所有线程共享。 堆空间被划分为新生代和老年代，新生代进一步划分为Eden和Surivor区，如下图所示。</p><p><img src="https://mic-blob-bucket.oss-cn-beijing.aliyuncs.com/202110171350001.png" alt="image-20210623205840226"></p><p>新生代和老年代的比例是1：2，也就是新生代会占1/3的堆空间，老年代会占2/3的堆空间。 另外，在新生代中，空间占比为Eden:Surivor0:Surivor1=8:1:1 。 举个例子来说，如果eden区内存大小是40M，那么两个Survivor区分别是占5M，整个新生代就是50M，然后计算出老年代的内存大小是100M，也就是说堆空间的总内存大小是150M。</p><blockquote><p>可以通过 java -XX:PrintFlagsFinal -version查看默认参数</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">uintx InitialSurvivorRatio                      = 8</span><br><span class="line">uintx NewRatio                                  = 2</span><br></pre></td></tr></table></figure><p>InitialSurvivorRatio:  新生代Eden/Survivor空间的初始比例</p><p>NewRatio ： Old区/Young区的内存比例</p></blockquote><p>堆内存的具体工作原理是：</p><ul><li>绝大部分的对象被创建之后，会保存在Eden区，当Eden区满了的时候，就会触发YGC（Young GC），大部分对象会被回收掉，如果还有活着的对象，就拷贝到Survivor0，这时Eden区被清空。</li><li>如果后续再次触发YGC，活着的对象Eden+Survivor0中的对象拷贝到Survivor1区， 这时Eden和Survivor0都会被清空</li><li>接着再触发YGC，Eden+Survivor1中的对象会被拷贝到Survivor0区，一直这么循环，直到对象的年龄达到阈值，则放入到老年代。（之所以这么设计，是因为Eden区的大部分对象会被回收）</li><li>Survivor区装不下的对象会直接进入到老年代</li><li>老年代满了，会触发Full GC。</li></ul><blockquote><p>GC标记-清除算法 在执行过程中暂停其他线程??</p></blockquote><p><img src="https://mic-blob-bucket.oss-cn-beijing.aliyuncs.com/202110171350497.png" alt="image-20210623214030533"></p><h4 id="程序计数器"><a href="#程序计数器" class="headerlink" title="程序计数器"></a>程序计数器</h4><p>程序计数器是用来记录各个线程执行的字节码地址等，当线程发生上下文切换时，需要依靠这个来记住当前执行的位置，当下次恢复执行后要沿着上一次执行的位置继续执行。</p><h4 id="方法区"><a href="#方法区" class="headerlink" title="方法区"></a>方法区</h4><p>方法区是逻辑上的概念，在HotSpot虚拟机的1.8版本中，它的具体实现就是元空间。</p><p>方法区主要用来存放已经被虚拟机加载的类相关信息，包括类元信息、运行时常量池、字符串常量池，类信息又包括类的版本、字段、方法、接口和父类信息等。</p><p>方法区和堆空间类似，它是一个共享内存区域，所以方法区是属于线程共享的。</p><h4 id="本地方发栈和虚拟机栈"><a href="#本地方发栈和虚拟机栈" class="headerlink" title="本地方发栈和虚拟机栈"></a>本地方发栈和虚拟机栈</h4><p>Java虚拟机栈是线程私有的内存空间，当创建一个线程时，会在虚拟机中申请一个线程栈，用来保存方法的局部变量、操作数栈、动态链接方法等信息。每一个方法的调用都伴随这栈帧的入栈操作，当一个方法返回之后，就是栈帧的出栈操作。</p><p>本地方法栈和虚拟机栈类似，本地方法栈是用来管理本地方法的调用，也就是native方法。</p><h4 id="JVM内存应该怎么设置"><a href="#JVM内存应该怎么设置" class="headerlink" title="JVM内存应该怎么设置"></a>JVM内存应该怎么设置</h4><p>了解了上述基本信息之后，那么JVM中内存应该如何设置呢？有哪些参数来设置？</p><p>而在JVM中，要配置的几个核心参数无非是。</p><ul><li><p><code>-Xms</code>，Java堆内存大小</p></li><li><p><code>-Xmx</code>，Java最大堆内存大小</p></li><li><p><code>-Xmn</code>，Java堆内存中的新生代大小，扣除新生代剩下的就是老年代内存</p><p>新生代内存设置过小会频繁触发Minor GC，频繁触发GC会影响系统的稳定性</p></li><li><p><code>-XX:MetaspaceSize</code>，元空间大小， 128M</p></li><li><p><code>-XX:MaxMetaspaceSize</code>，最大云空间大小 （如果没有指定这两个参数，元空间会在运行时根据需要动态调整。）  256M</p><blockquote><p>一个新系统的元空间，基本上没办法有一个测算的方法，一般设置几百兆就够用，因为这里面主要存放一些类信息。</p></blockquote></li><li><p><code>-Xss</code>，线程栈内存大小，这个基本上不需要预估，设置512KB到1M就行，因为值越小，能够分配的线程数越多。</p></li></ul><p>JVM内存的大小，取决于机器的配置，比如一个2核4G的服务器，能够分配给JVM进程也就2G左右，因为机器本身也需要内存，而且机器上还运行了其他的进程也需要占内存。而这2G还得分配给栈内存、堆内存、元空间，那堆内存能够得到的也就1G左右，然后堆内存还要分新生代、老年代。</p><h3 id="Tomcat本身的配置"><a href="#Tomcat本身的配置" class="headerlink" title="Tomcat本身的配置"></a>Tomcat本身的配置</h3><blockquote><p><a href="http://tomcat.apache.org/tomcat-8.0-doc/config/http.html">http://tomcat.apache.org/tomcat-8.0-doc/config/http.html</a></p></blockquote><blockquote><p>The maximum number of request processing threads to be created by this <strong>Connector</strong>, which therefore determines the maximum number of simultaneous requests that can be handled. If not specified, this attribute is set to 200. If an executor is associated with this connector, this attribute is ignored as the connector will execute tasks using the executor rather than an internal thread pool. Note that if an executor is configured any value set for this attribute will be recorded correctly but it will be reported (e.g. via JMX) as <code>-1</code> to make clear that it is not used.</p></blockquote><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">server:</span></span><br><span class="line">  <span class="attr">tomcat:</span></span><br><span class="line">    <span class="attr">uri-encoding:</span> <span class="string">UTF-8</span></span><br><span class="line">    <span class="comment">#最大工作线程数，默认200, 4核8g内存，线程数经验值800</span></span><br><span class="line">    <span class="comment">#操作系统做线程之间的切换调度是有系统开销的，所以不是越多越好。</span></span><br><span class="line">    <span class="attr">max-threads:</span> <span class="number">1000</span></span><br><span class="line">    <span class="comment"># 等待队列长度，默认100，</span></span><br><span class="line">    <span class="attr">accept-count:</span> <span class="number">1000</span></span><br><span class="line">    <span class="attr">max-connections:</span> <span class="number">20000</span></span><br><span class="line">    <span class="comment"># 最小工作空闲线程数，默认10, 适当增大一些，以便应对突然增长的访问量</span></span><br><span class="line">    <span class="attr">min-spare-threads:</span> <span class="number">100</span></span><br></pre></td></tr></table></figure><ul><li><p><strong>accept-count:</strong> 最大等待数，当调用HTTP请求数达到tomcat的最大线程数时，还有新的HTTP请求到来，这时tomcat会将该请求放在等待队列中，这个acceptCount就是指能够接受的最大等待数，默认100。如果等待队列也被放满了，这个时候再来新的请求就会被tomcat拒绝（connection refused）</p></li><li><p><strong>maxThreads：</strong>最大线程数，每一次HTTP请求到达Web服务，tomcat都会创建一个线程来处理该请求，那么最大线程数决定了Web服务容器可以同时处理多少个请求。maxThreads默认200，肯定建议增加。但是，增加线程是有成本的，更多的线程，不仅仅会带来更多的线程上下文切换成本，而且意味着带来更多的内存消耗。JVM中默认情况下在创建新线程时会分配大小为1M的线程栈，所以，更多的线程异味着需要更多的内存。线程数的经验值为：1核2g内存为200，线程数经验值200；4核8g内存，线程数经验值800。</p></li><li><p><strong>maxConnections</strong>，最大连接数，这个参数是指在同一时间，tomcat能够接受的最大连接数。对于Java的阻塞式BIO，默认值是maxthreads的值；如果在BIO模式使用定制的Executor执行器，默认值将是执行器中maxthreads的值。对于Java 新的NIO模式，maxConnections 默认值是10000。对于windows上APR/native IO模式，maxConnections默认值为8192</p><p>如果设置为-1，则禁用maxconnections功能，表示不限制tomcat容器的连接数。<br><strong>maxConnections和accept-count的关系为：当连接数达到最大值maxConnections后，系统会继续接收连接，但不会超过acceptCount的值。</strong></p></li></ul><h3 id="1-3-4-应用带来的压力"><a href="#1-3-4-应用带来的压力" class="headerlink" title="1.3.4 应用带来的压力"></a>1.3.4 应用带来的压力</h3><p>前面我们分析过，NIOEndPoint接收到客户端请求连接后，会生成一个SocketProcessor任务给到线程池去处理，SocketProcessor中的run方法会调用HttpProcessor组件去解析应用层的协议，并生成Request对象。最后调用Adapter的Service方法，将请求传递到容器中。</p><p>容器主要负责内部的处理工作，也就是当前置的连接器通过Socket获取到信息之后，得到一个Servlet请求，而容器就是负责处理Servlet请求。</p><p>Tomcat使用Mapper组件将用户请求的URL定位到一个具体的Serlvet，然后Spring中的DispatcherServlet拦截到该Servlet请求后，基于Spring本身的Mapper映射定位到我们具体的Controller中。</p><p>到了Controller之后，对于我们的业务来说，才是一个请求真正的开始，Controller调用Service、Service调用dao，完成数据库操作之后，讲请求原路返回给到客户端，完成一次整体的会话。也就是说，Controller中的业务逻辑处理耗时，对于整个容器的并发来说也会受到影响。</p><p><img src="https://mic-blob-bucket.oss-cn-beijing.aliyuncs.com/202110171350088.png" alt="image-20210622151107514"></p><h2 id="服务器数量评估"><a href="#服务器数量评估" class="headerlink" title="服务器数量评估"></a>服务器数量评估</h2><p>通过上述分析，我们假设一个tomcat节点的QPS=500，如果要支撑到高峰时期的QPS=18000，那么需要40台服务器，这四台服务器需要通过Nginx软件负载均衡，进行请求分发，Nginx的性能很好，官方给的说明是Nginx处理静态文件的并发能够达到5W/s。另外Nginx由于不能单点，我们可以采用LVS对Nginx做负载均衡，LVS（Linux VirtualServer），它是采用IP负载均衡技术实现负载均衡。</p><p><img src="https://mic-blob-bucket.oss-cn-beijing.aliyuncs.com/202110171350886.png" alt="image-20210622220213652"></p><p>通过这样的一组架构，我们当前服务端是能够同时承接QPS=18000，但是还不够，再回到前面我们说的两个公式。</p><ul><li><p>QPS=并发量/平均响应时间</p></li><li><p>并发量=QPS*平均响应时间</p></li></ul><p>假设我们的RT是3s，那么意味着服务器端的并发数=18000*3=54000，也就是同时有54000个连接打到服务器端，所以服务端需要同时支持的连接数为54000，这个我们在前文说过如何进行配置。如果RT越大，那么意味着堆积的链接越多，而这些连接会占用内存资源/CPU资源等，容易造成系统崩溃的现象。同时，当链接数超过阈值时，后续的请求无法进来，用户会得到一个请求超时的结果，这显然不是我们所希望看到的，所以我们必须要缩短RT的值。</p>]]></content>
      
      
      <categories>
          
          <category> 架构设计 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 高并发 </tag>
            
            <tag> 架构设计 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/posts/1243066710/"/>
      <url>/posts/1243066710/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
